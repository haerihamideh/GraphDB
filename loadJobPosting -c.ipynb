{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Populate the Job Posting Ontology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# required libraries\n",
    "import pandas as pd\n",
    "import os\n",
    "from pathlib import Path\n",
    "# Load the required libraries\n",
    "from rdflib import Graph, Literal, RDF, URIRef, Namespace, BNode, OWL, RDFS\n",
    "# rdflib knows about some namespaces, like FOAF\n",
    "from rdflib.namespace import FOAF, XSD\n",
    "# CHECK DATE \n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Navid\\Documents\\GitHub\n",
      "data/job_posting.csv\n"
     ]
    }
   ],
   "source": [
    "# parameters and URLs\n",
    "path = str(Path(os.path.abspath(os.getcwd())).parent.absolute())\n",
    "print(path)\n",
    "jobPostingUrl = 'data/job_posting.csv'\n",
    "print(jobPostingUrl)\n",
    "\n",
    "companiesUrl = path + '/GraphDB/data/company_details/companies.csv'\n",
    "company_industriesUrl = path + '/GraphDB/data/company_details/company_industries.csv'\n",
    "company_specialitiesUrl = path + '/GraphDB/data/company_details/company_specialities.csv'\n",
    "specialitiesUrl = path + '/GraphDB/data/company_details/specialities.csv'\n",
    "employee_countsUrl = path + '/GraphDB/data/company_details/employee_counts.csv'\n",
    "c = path + '/GraphDB/data/company_details/specialities.csv'\n",
    "\n",
    "benefitsUrl = path + '/GraphDB/data/job_details/benefits.csv'\n",
    "job_industriesUrl = path + '/GraphDB/data/job_details/job_industries.csv'\n",
    "job_skillsUrl = path + '/GraphDB/data/job_details/job_skills.csv'\n",
    "salariesUrl = path + '/GraphDB/data/job_details/salaries.csv'\n",
    "\n",
    "industriesUrl = path + '/GraphDB/data/maps/industries.csv'\n",
    "skillsUrl = path + '/GraphDB/data/maps/skills.csv'\n",
    "\n",
    "\n",
    "# country codes\n",
    "countriesURL = path + '/GraphDB/data/countries/all.csv'\n",
    "\n",
    "# saving folder\n",
    "savePath =  path + '/data/rdf/linkedinDB/' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct the country and the movie ontology namespaces not known by RDFlib\n",
    "CNS = Namespace(\"http://eulersharp.sourceforge.net/2003/03swap/countries#\")\n",
    "LNJP = Namespace(\"http://www.dei.unipd.it/database2/LinkedinJobPosting#\")\n",
    "SKOS = Namespace(\"https://www.w3.org/2009/08/skos-reference/skos-owl1-dl.rdf\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Job Posting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CSV files in memory\n",
    "jobPosting = pd.read_csv(jobPostingUrl, sep=',', index_col='job_id')\n",
    "#print(jobPosting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create the graph\n",
    "g = Graph()\n",
    "\n",
    "# Bind the namespaces to a prefix for more readable output\n",
    "g.bind(\"foaf\", FOAF)\n",
    "g.bind(\"xsd\", XSD)\n",
    "g.bind(\"countries\", CNS)\n",
    "g.bind(\"lnjp\", LNJP)\n",
    "g.bind(\"skos\", SKOS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- saving serialization ---\n",
      "RDF data exported to job_postings.ttl\n",
      "Serialization saved successfully.\n"
     ]
    }
   ],
   "source": [
    "# Iterate over the job postings DataFrame\n",
    "for index, row in jobPosting.iterrows():\n",
    "    # Create the JobPosting node with a URI\n",
    "    jobPostingId = \"job_posting\" + str(index)\n",
    "    JobPosting = URIRef(LNJP[jobPostingId])\n",
    "    g.add((JobPosting, RDF.type, LNJP.JobPosting))\n",
    "\n",
    "\n",
    "    #---- for check ---\n",
    "\n",
    "    # TransfermarktProfile = URIRef(row['url'])\n",
    "    \n",
    "    # # Add triples using store's add() method.\n",
    "    # g.add((Player, RDF.type, SO.Player))\n",
    "    # g.add((TransfermarktProfile, RDF.type, SO.TransfermarktProfile))\n",
    "    # g.add((TransfermarktProfile, SO['isAbout'], Player))\n",
    "\n",
    "    #------------------\n",
    "\n",
    "    # Add data properties\n",
    "    g.add((JobPosting, LNJP.job_id, Literal(index, datatype=XSD.int)))\n",
    "    g.add((JobPosting, LNJP.title, Literal(row['title'], datatype=XSD.string)))\n",
    "    g.add((JobPosting, LNJP.description, Literal(row['description'], datatype=XSD.string)))\n",
    "    g.add((JobPosting, LNJP.max_salary, Literal(row['max_salary'], datatype=XSD.int)))\n",
    "    g.add((JobPosting, LNJP.med_salary, Literal(row['med_salary'], datatype=XSD.int)))\n",
    "    g.add((JobPosting, LNJP.min_salary, Literal(row['min_salary'], datatype=XSD.int)))\n",
    "    g.add((JobPosting, LNJP.pay_period, Literal(row['pay_period'], datatype=XSD.string)))\n",
    "    g.add((JobPosting, LNJP.location, Literal(row['location'], datatype=XSD.string)))\n",
    "    g.add((JobPosting, LNJP.applies, Literal(row['applies'], datatype=XSD.int)))\n",
    "    g.add((JobPosting, LNJP.original_listed_time, Literal(row['original_listed_time'], datatype=XSD.dateTime)))\n",
    "    g.add((JobPosting, LNJP.remote_allowed, Literal(row['remote_allowed'], datatype=XSD.int)))\n",
    "    g.add((JobPosting, LNJP.views, Literal(row['views'], datatype=XSD.int)))\n",
    "    g.add((JobPosting, LNJP.job_posting_url, Literal(row['job_posting_url'], datatype=XSD.string)))\n",
    "    g.add((JobPosting, LNJP.application_url, Literal(row['application_url'], datatype=XSD.string)))\n",
    "    g.add((JobPosting, LNJP.application_type, Literal(row['application_type'], datatype=XSD.string)))\n",
    "    g.add((JobPosting, LNJP.expiry, Literal(row['expiry'], datatype=XSD.dateTime)))\n",
    "    g.add((JobPosting, LNJP.closed_time, Literal(row['closed_time'], datatype=XSD.dateTime)))\n",
    "    g.add((JobPosting, LNJP.formatted_experience_level, Literal(row['formatted_experience_level'], datatype=XSD.string)))\n",
    "    g.add((JobPosting, LNJP.skills_desc, Literal(row['skills_desc'], datatype=XSD.string)))\n",
    "    g.add((JobPosting, LNJP.listed_time, Literal(row['listed_time'], datatype=XSD.dateTime)))\n",
    "    g.add((JobPosting, LNJP.posting_domain, Literal(row['posting_domain'], datatype=XSD.string)))\n",
    "    g.add((JobPosting, LNJP.sponsored, Literal(row['sponsored'], datatype=XSD.int)))\n",
    "    g.add((JobPosting, LNJP.work_type, Literal(row['work_type'], datatype=XSD.string)))\n",
    "    g.add((JobPosting, LNJP.currency, Literal(row['currency'], datatype=XSD.string)))\n",
    "    g.add((JobPosting, LNJP.compensation_type, Literal(row['compensation_type'], datatype=XSD.string)))\n",
    "\n",
    "    Company = URIRef(LNJP[\"company_\" + str(row['company_id'])])\n",
    "    g.add((JobPosting, LNJP['hasCompany'], Company))\n",
    "# terminated\n",
    "# jobSkillsAbr = \"skill_\" + str(row['abr']) # You should replace this with the actual ID or URI\n",
    "# JobSkills = URIRef(LNJP[jobSkillsAbr])\n",
    "# print(JobSkills)\n",
    "\n",
    "\n",
    "# Serialize the RDF graph to Turtle format\n",
    "turtle_file_path = 'job_postings.ttl'\n",
    "\n",
    "# Check if the file already exists\n",
    "if os.path.exists(turtle_file_path):\n",
    "    user_input = input(f\"The file '{turtle_file_path}' already exists. Do you want to rewrite it? (y/n): \").lower()\n",
    "    \n",
    "    if user_input != 'y':\n",
    "        print(\"Serialization not saved. Exiting.\")\n",
    "        # You might want to add additional logic or exit the script if the user chooses not to overwrite the file.\n",
    "        exit()\n",
    "\n",
    "# If the file doesn't exist or the user chose to overwrite, proceed with saving the serialization\n",
    "print(\"--- saving serialization ---\")\n",
    "g.serialize(destination=turtle_file_path, format='turtle')\n",
    "\n",
    "print(f\"RDF data exported to {turtle_file_path}\")\n",
    "print(\"Serialization saved successfully.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Skills"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skills DataFrame read successfully.\n"
     ]
    }
   ],
   "source": [
    "# Load the CSV files in memory\n",
    "skills = pd.read_csv(skillsUrl, sep=',', index_col='skill_abr', keep_default_na=False, na_values=['_'])\n",
    "assert not skills.empty, \"Skills DataFrame is empty. Check the CSV file or URL.\"\n",
    "\n",
    "# Print a message if the test passes\n",
    "print(\"Skills DataFrame read successfully.\")\n",
    "#create the graph\n",
    "g = Graph()\n",
    "\n",
    "# Bind the namespaces to a prefix for more readable output\n",
    "g.bind(\"foaf\", FOAF)\n",
    "g.bind(\"xsd\", XSD)\n",
    "g.bind(\"countries\", CNS)\n",
    "g.bind(\"lnjp\", LNJP)\n",
    "g.bind(\"skos\", SKOS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- saving serialization ---\n",
      "RDF data exported to skills.ttl\n",
      "Serialization saved successfully.\n"
     ]
    }
   ],
   "source": [
    "# Assuming you have a DataFrame called 'skillsDataFrame' with columns 'skill_abr' and 'skill_name'\n",
    "# Iterate over all rows in the skills DataFrame\n",
    "for skill_abr, row in skills.iterrows():\n",
    "    # Create Skill node with a URI\n",
    "    skillId = \"skill_\" + str(skill_abr)\n",
    "    Skill = URIRef(LNJP[skillId])\n",
    "    g.add((Skill, RDF.type, SKOS.Concept))\n",
    "    g.add((Skill, RDF.type, LNJP.Skill))\n",
    "\n",
    "    # Add skill information as data properties\n",
    "    g.add((Skill, LNJP.skill_abr, Literal(skill_abr, datatype=XSD.string)))\n",
    "    g.add((Skill, LNJP.skill_name, Literal(row['skill_name'], datatype=XSD.string)))\n",
    "\n",
    "    if(row['broader_concept']):\n",
    "        broaderSkillId = \"skill_\" + str(row['broader_concept'])\n",
    "        broaderSkill = URIRef(LNJP[broaderSkillId])\n",
    "        g.add((Skill, SKOS.broader, broaderSkill))\n",
    "    if(row['narrower_concept']):\n",
    "        narrowerSkillId = \"skill_\" + str(row['narrower_concept'])\n",
    "        narrowerSkill = URIRef(LNJP[narrowerSkillId])\n",
    "        g.add((Skill, SKOS.narrower, narrowerSkill))\n",
    "        \n",
    "    if(row['related_concept']):\n",
    "        relatedSkillId = \"skill_\" + str(row['related_concept'])\n",
    "        relatedSkill = URIRef(LNJP[relatedSkillId])\n",
    "        g.add((Skill, SKOS.related, relatedSkill))\n",
    "    \n",
    "\n",
    "# Serialize the RDF graph to Turtle format\n",
    "turtle_file_path = 'skills.ttl'\n",
    "\n",
    "# Check if the file already exists\n",
    "if os.path.exists(turtle_file_path):\n",
    "    user_input = input(f\"The file '{turtle_file_path}' already exists. Do you want to rewrite it? (y/n): \").lower()\n",
    "\n",
    "    if user_input != 'y':\n",
    "        print(\"Serialization not saved. Exiting.\")\n",
    "        # You might want to add additional logic or exit the script if the user chooses not to overwrite the file.\n",
    "        exit()\n",
    "\n",
    "# If the file doesn't exist or the user chose to overwrite, proceed with saving the serialization\n",
    "print(\"--- saving serialization ---\")\n",
    "g.serialize(destination=turtle_file_path, format='turtle')\n",
    "\n",
    "print(f\"RDF data exported to {turtle_file_path}\")\n",
    "print(\"Serialization saved successfully.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## JOB_SKILLS JOIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skills DataFrame read successfully.\n"
     ]
    }
   ],
   "source": [
    "# Load the CSV files in memory\n",
    "job_skills = pd.read_csv(job_skillsUrl, sep=',', index_col='job_id')\n",
    "assert not job_skills.empty, \"Skills DataFrame is empty. Check the CSV file or URL.\"\n",
    "\n",
    "# Print a message if the test passes\n",
    "print(\"Skills DataFrame read successfully.\")\n",
    "#create the graph\n",
    "g = Graph()\n",
    "\n",
    "# Bind the namespaces to a prefix for more readable output\n",
    "g.bind(\"foaf\", FOAF)\n",
    "g.bind(\"xsd\", XSD)\n",
    "g.bind(\"countries\", CNS)\n",
    "g.bind(\"lnjp\", LNJP)\n",
    "g.bind(\"skos\", SKOS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- saving serialization ---\n",
      "RDF data exported to job_skills_join.ttl\n",
      "Serialization saved successfully.\n"
     ]
    }
   ],
   "source": [
    "# Assuming you have a DataFrame called 'skillsDataFrame' with columns 'skill_abr' and 'skill_name'\n",
    "# Iterate over all rows in the skills DataFrame\n",
    "for index, row in job_skills.iterrows():\n",
    "    # Create Skill node with a URI\n",
    "    # skillId = \"skill_\" + str(skill_abr)\n",
    "    \n",
    "    jobPostingId = \"job_posting\" + str(index)\n",
    "    JobPosting = URIRef(LNJP[jobPostingId])\n",
    "    Skill = URIRef(LNJP[row['skill_abr']])\n",
    "    g.add((JobPosting, LNJP['hasSkill'], Skill))\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "# Serialize the RDF graph to Turtle format\n",
    "turtle_file_path = 'job_skills_join.ttl'\n",
    "\n",
    "# Check if the file already exists\n",
    "if os.path.exists(turtle_file_path):\n",
    "    user_input = input(f\"The file '{turtle_file_path}' already exists. Do you want to rewrite it? (y/n): \").lower()\n",
    "\n",
    "    if user_input != 'y':\n",
    "        print(\"Serialization not saved. Exiting.\")\n",
    "        # You might want to add additional logic or exit the script if the user chooses not to overwrite the file.\n",
    "        exit()\n",
    "\n",
    "# If the file doesn't exist or the user chose to overwrite, proceed with saving the serialization\n",
    "print(\"--- saving serialization ---\")\n",
    "g.serialize(destination=turtle_file_path, format='turtle')\n",
    "\n",
    "print(f\"RDF data exported to {turtle_file_path}\")\n",
    "print(\"Serialization saved successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Industry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Industries DataFrame read successfully.\n"
     ]
    }
   ],
   "source": [
    "# Load the CSV files in memory\n",
    "industries = pd.read_csv(industriesUrl, sep=',', index_col='industry_id', keep_default_na=False, na_values=['_'])\n",
    "assert not industries.empty, \"Industries DataFrame is empty. Check the CSV file or URL.\"\n",
    "\n",
    "# Print a message if the test passes\n",
    "print(\"Industries DataFrame read successfully.\")\n",
    "#create the graph\n",
    "g = Graph()\n",
    "\n",
    "# Bind the namespaces to a prefix for more readable output\n",
    "g.bind(\"foaf\", FOAF)\n",
    "g.bind(\"xsd\", XSD)\n",
    "g.bind(\"countries\", CNS)\n",
    "g.bind(\"lnjp\", LNJP)\n",
    "g.bind(\"skos\", SKOS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- saving serialization ---\n",
      "RDF data exported to industries.ttl\n",
      "Serialization saved successfully.\n"
     ]
    }
   ],
   "source": [
    "# Assuming you have a DataFrame called 'industiesDataFrom' with columns 'industry_id' and 'industry_name'\n",
    "# Iterate over all rows in the industries DataFrame\n",
    "for index, row in industries.iterrows():\n",
    "    # Create Skill node with a URI\n",
    "    industryId = \"industry_\" + str(index)\n",
    "    Industry = URIRef(LNJP[industryId])\n",
    "    g.add((Industry, RDF.type, LNJP.Industry))\n",
    "\n",
    "    # Add skill information as data properties\n",
    "    g.add((Industry, LNJP['industry_id'], Literal(index, datatype=XSD.string)))\n",
    "    g.add((Industry, LNJP['industry_name'], Literal(row['industry_name'], datatype=XSD.string)))\n",
    "    \n",
    "\n",
    "# Serialize the RDF graph to Turtle format\n",
    "turtle_file_path = 'industries.ttl'\n",
    "\n",
    "# Check if the file already exists\n",
    "if os.path.exists(turtle_file_path):\n",
    "    user_input = input(f\"The file '{turtle_file_path}' already exists. Do you want to rewrite it? (y/n): \").lower()\n",
    "\n",
    "    if user_input != 'y':\n",
    "        print(\"Serialization not saved. Exiting.\")\n",
    "        # You might want to add additional logic or exit the script if the user chooses not to overwrite the file.\n",
    "        exit()\n",
    "\n",
    "# If the file doesn't exist or the user chose to overwrite, proceed with saving the serialization\n",
    "print(\"--- saving serialization ---\")\n",
    "g.serialize(destination=turtle_file_path, format='turtle')\n",
    "\n",
    "print(f\"RDF data exported to {turtle_file_path}\")\n",
    "print(\"Serialization saved successfully.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Job_Industries join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skills DataFrame read successfully.\n"
     ]
    }
   ],
   "source": [
    "# Load the CSV files in memory\n",
    "job_industries = pd.read_csv(job_industriesUrl, sep=',', index_col='job_id')\n",
    "assert not job_industries.empty, \"Skills DataFrame is empty. Check the CSV file or URL.\"\n",
    "\n",
    "# Print a message if the test passes\n",
    "print(\"Skills DataFrame read successfully.\")\n",
    "#create the graph\n",
    "g = Graph()\n",
    "\n",
    "# Bind the namespaces to a prefix for more readable output\n",
    "g.bind(\"foaf\", FOAF)\n",
    "g.bind(\"xsd\", XSD)\n",
    "g.bind(\"countries\", CNS)\n",
    "g.bind(\"lnjp\", LNJP)\n",
    "g.bind(\"skos\", SKOS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- saving serialization ---\n",
      "RDF data exported to job_industries_join.ttl\n",
      "Serialization saved successfully.\n"
     ]
    }
   ],
   "source": [
    "# Assuming you have a DataFrame called 'skillsDataFrame' with columns 'skill_abr' and 'skill_name'\n",
    "# Iterate over all rows in the skills DataFrame\n",
    "for index, row in job_industries.iterrows():\n",
    "    # Create Skill node with a URI\n",
    "    # skillId = \"skill_\" + str(skill_abr)\n",
    "    \n",
    "    jobPosting_id = \"job_posting\" + str(index)\n",
    "    JobPosting = URIRef(LNJP[jobPosting_id])\n",
    "    Industry = URIRef(LNJP[\"industry_\" + str(row['industry_id'])])\n",
    "    g.add((JobPosting, LNJP['hasIndustryType'], Industry))\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "# Serialize the RDF graph to Turtle format\n",
    "turtle_file_path = 'job_industries_join.ttl'\n",
    "\n",
    "# Check if the file already exists\n",
    "if os.path.exists(turtle_file_path):\n",
    "    user_input = input(f\"The file '{turtle_file_path}' already exists. Do you want to rewrite it? (y/n): \").lower()\n",
    "\n",
    "    if user_input != 'y':\n",
    "        print(\"Serialization not saved. Exiting.\")\n",
    "        # You might want to add additional logic or exit the script if the user chooses not to overwrite the file.\n",
    "        exit()\n",
    "\n",
    "# If the file doesn't exist or the user chose to overwrite, proceed with saving the serialization\n",
    "print(\"--- saving serialization ---\")\n",
    "g.serialize(destination=turtle_file_path, format='turtle')\n",
    "\n",
    "print(f\"RDF data exported to {turtle_file_path}\")\n",
    "print(\"Serialization saved successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Company"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Companies DataFrame read successfully.\n"
     ]
    }
   ],
   "source": [
    "# Load the CSV files in memory\n",
    "companies = pd.read_csv(companiesUrl, sep=',', index_col='company_id', keep_default_na=False, na_values=['_'])\n",
    "assert not companies.empty, \"Companies DataFrame is empty. Check the CSV file or URL.\"\n",
    "\n",
    "# Print a message if the test passes\n",
    "print(\"Companies DataFrame read successfully.\")\n",
    "#create the graph\n",
    "g = Graph()\n",
    "\n",
    "# Bind the namespaces to a prefix for more readable output\n",
    "g.bind(\"foaf\", FOAF)\n",
    "g.bind(\"xsd\", XSD)\n",
    "g.bind(\"countries\", CNS)\n",
    "g.bind(\"lnjp\", LNJP)\n",
    "g.bind(\"skos\", SKOS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- saving serialization ---\n",
      "RDF data exported to companies.ttl\n",
      "Serialization saved successfully.\n"
     ]
    }
   ],
   "source": [
    "# Define classes and properties\n",
    "largeCompany = LNJP.largeCompany\n",
    "company_size_property = LNJP['company_size']\n",
    "\n",
    "BN0 = BNode()\n",
    "BN1 = BNode()\n",
    "BN2 = BNode()\n",
    "BN3 = BNode()\n",
    "BN4 = BNode()\n",
    "BN5 = BNode()\n",
    "BN6 = BNode()\n",
    "BN7 = BNode()\n",
    "BN8 = BNode()\n",
    "\n",
    "# Define the class and its equivalent class\n",
    "g.add((largeCompany, RDF.type, OWL.Class))\n",
    "g.add((largeCompany, RDFS.subClassOf, LNJP.Company))\n",
    "\n",
    "g.add((largeCompany, OWL.equivalentClass, BN0))\n",
    "\n",
    "g.add((BN0, RDF.type, OWL.Class))\n",
    "g.add((BN0, OWL.intersectionOf, BN1))\n",
    "\n",
    "g.add((BN1, RDF.type, RDF.List))\n",
    "g.add((BN1, RDF.first, LNJP.Company))\n",
    "g.add((BN1, RDF.rest, BN2))\n",
    "\n",
    "g.add((BN2, RDF.type, RDF.List))\n",
    "g.add((BN2, RDF.first, BN3))\n",
    "g.add((BN2, RDF.rest, RDF.nil))\n",
    "\n",
    "g.add((BN3, RDF.type, OWL.Restriction))\n",
    "g.add((BN3, OWL.onProperty, company_size_property))\n",
    "g.add((BN3, OWL.someValuesFrom, BN4))\n",
    "\n",
    "g.add((BN4, RDF.type, RDFS.Datatype))\n",
    "g.add((BN4, OWL.onDatatype, XSD.integer))\n",
    "g.add((BN4, OWL.withRestrictions, BN5))\n",
    "\n",
    "g.add((BN5, RDF.type, RDF.List))\n",
    "g.add((BN6, RDF.type, RDF.List))\n",
    "g.add((BN7, RDF.type, RDF.List))\n",
    "g.add((BN8, RDF.type, RDF.List))\n",
    "\n",
    "g.add((BN5, RDF.first, BN6))\n",
    "g.add((BN5, RDF.rest, BN7))\n",
    "\n",
    "g.add((BN7, RDF.first, BN8))\n",
    "g.add((BN7, RDF.rest, RDF.nil))\n",
    "\n",
    "g.add((BN6, XSD.minInclusive, Literal(\"6\", datatype=XSD.integer)))\n",
    "g.add((BN8, XSD.maxInclusive, Literal(\"7\", datatype=XSD.integer)))\n",
    "\n",
    "# Assuming you have a DataFrame called 'companiesDataFrom' with columns 'company_id', 'name', 'description', 'company_size', 'state', 'country', 'city', 'zip_code', 'address' and 'url'\n",
    "# Iterate over all rows in the companies DataFrame\n",
    "for index, row in companies.iterrows():\n",
    "    # Create Company node with a URI\n",
    "    companyId = \"company_\" + str(index)\n",
    "    Company = URIRef(LNJP[companyId])\n",
    "    g.add((Company, RDF.type, LNJP.Company))\n",
    "\n",
    "    # Add Company information as data properties\n",
    "    g.add((Company, LNJP['company_id'], Literal(index, datatype=XSD.string)))\n",
    "    g.add((Company, LNJP['name'], Literal(row['name'], datatype=XSD.string)))\n",
    "    g.add((Company, LNJP['description'], Literal(row['description'], datatype=XSD.string)))\n",
    "    if not pd.isnull(row['company_size']) and row['company_size'] != \"\":\n",
    "        g.add((Company, LNJP['company_size'], Literal(row['company_size'], datatype=XSD.integer)))\n",
    "    g.add((Company, LNJP['state'], Literal(row['state'], datatype=XSD.string)))\n",
    "    g.add((Company, LNJP['city'], Literal(row['city'], datatype=XSD.string)))\n",
    "    g.add((Company, LNJP['zip_code'], Literal(row['zip_code'], datatype=XSD.string)))\n",
    "    g.add((Company, LNJP['address'], Literal(row['address'], datatype=XSD.string)))\n",
    "    g.add((Company, LNJP['url'], Literal(row['url'], datatype=XSD.string)))\n",
    "\n",
    "    Country = URIRef(CNS[row['country']])\n",
    "    g.add((Company, LNJP['hasCountry'], Country))\n",
    "# Serialize the RDF graph to Turtle format\n",
    "turtle_file_path = 'companies.ttl'\n",
    "\n",
    "# Check if the file already exists\n",
    "if os.path.exists(turtle_file_path):\n",
    "    user_input = input(f\"The file '{turtle_file_path}' already exists. Do you want to rewrite it? (y/n): \").lower()\n",
    "\n",
    "    if user_input != 'y':\n",
    "        print(\"Serialization not saved. Exiting.\")\n",
    "        # You might want to add additional logic or exit the script if the user chooses not to overwrite the file.\n",
    "        exit()\n",
    "\n",
    "# If the file doesn't exist or the user chose to overwrite, proceed with saving the serialization\n",
    "print(\"--- saving serialization ---\")\n",
    "g.serialize(destination=turtle_file_path, format='turtle')\n",
    "\n",
    "print(f\"RDF data exported to {turtle_file_path}\")\n",
    "print(\"Serialization saved successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Company_Industries join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Navid\\Documents\\GitHub\\GraphDB\\loadJobPosting -c.ipynb Cell 25\u001b[0m line \u001b[0;36m2\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Navid/Documents/GitHub/GraphDB/loadJobPosting%20-c.ipynb#X33sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# Load the CSV files in memory\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Navid/Documents/GitHub/GraphDB/loadJobPosting%20-c.ipynb#X33sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m company_industries \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mread_csv(company_industriesUrl, sep\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39m,\u001b[39m\u001b[39m'\u001b[39m, index_col\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mcompany_id\u001b[39m\u001b[39m'\u001b[39m, keep_default_na\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, na_values\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39m_\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Navid/Documents/GitHub/GraphDB/loadJobPosting%20-c.ipynb#X33sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mnot\u001b[39;00m company_industries\u001b[39m.\u001b[39mempty, \u001b[39m\"\u001b[39m\u001b[39mSkills DataFrame is empty. Check the CSV file or URL.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Navid/Documents/GitHub/GraphDB/loadJobPosting%20-c.ipynb#X33sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39m# Print a message if the test passes\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "# Load the CSV files in memory\n",
    "company_industries = pd.read_csv(company_industriesUrl, sep=',', index_col='company_id', keep_default_na=False, na_values=['_'])\n",
    "assert not company_industries.empty, \"Skills DataFrame is empty. Check the CSV file or URL.\"\n",
    "\n",
    "# Print a message if the test passes\n",
    "print(\"Skills DataFrame read successfully.\")\n",
    "#create the graph\n",
    "g = Graph()\n",
    "\n",
    "# Bind the namespaces to a prefix for more readable output\n",
    "g.bind(\"foaf\", FOAF)\n",
    "g.bind(\"xsd\", XSD)\n",
    "g.bind(\"countries\", CNS)\n",
    "g.bind(\"lnjp\", LNJP)\n",
    "g.bind(\"skos\", SKOS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- saving serialization ---\n",
      "RDF data exported to company_industries_join.ttl\n",
      "Serialization saved successfully.\n"
     ]
    }
   ],
   "source": [
    "# Assuming you have a DataFrame called 'skillsDataFrame' with columns 'skill_abr' and 'skill_name'\n",
    "# Iterate over all rows in the skills DataFrame\n",
    "for index, row in company_industries.iterrows():\n",
    "    # Create Skill node with a URI\n",
    "    # skillId = \"skill_\" + str(skill_abr)\n",
    "    \n",
    "    company_id = \"company_\" + str(index)\n",
    "    Company = URIRef(LNJP[company_id])\n",
    "    Industry = URIRef(LNJP[\"industry_\" + str(row['industry_id'])])\n",
    "    g.add((Company, LNJP['hasIndustry'], Industry))\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "# Serialize the RDF graph to Turtle format\n",
    "turtle_file_path = 'company_industries_join.ttl'\n",
    "\n",
    "# Check if the file already exists\n",
    "if os.path.exists(turtle_file_path):\n",
    "    user_input = input(f\"The file '{turtle_file_path}' already exists. Do you want to rewrite it? (y/n): \").lower()\n",
    "\n",
    "    if user_input != 'y':\n",
    "        print(\"Serialization not saved. Exiting.\")\n",
    "        # You might want to add additional logic or exit the script if the user chooses not to overwrite the file.\n",
    "        exit()\n",
    "\n",
    "# If the file doesn't exist or the user chose to overwrite, proceed with saving the serialization\n",
    "print(\"--- saving serialization ---\")\n",
    "g.serialize(destination=turtle_file_path, format='turtle')\n",
    "\n",
    "print(f\"RDF data exported to {turtle_file_path}\")\n",
    "print(\"Serialization saved successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Person Counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CSV files in memory\n",
    "employee_counts = pd.read_csv(employee_countsUrl, sep=',', index_col='company_record_id')\n",
    "\n",
    "# print(Company)\n",
    "\n",
    "#create the graph\n",
    "g = Graph()\n",
    "\n",
    "# Bind the namespaces to a prefix for more readable output\n",
    "g.bind(\"foaf\", FOAF)\n",
    "g.bind(\"xsd\", XSD)\n",
    "g.bind(\"countries\", CNS)\n",
    "g.bind(\"lnjp\", LNJP)\n",
    "g.bind(\"skos\", SKOS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- saving serialization ---\n",
      "RDF data exported to employee_counts.ttl\n",
      "Serialization saved successfully.\n"
     ]
    }
   ],
   "source": [
    "# Iterate over all rows in the companies DataFrame\n",
    "for index, row in employee_counts.iterrows():\n",
    "    # Create Company node with a URI\n",
    "    company_record_id = \"company_record_\" + str(index)\n",
    "    Record = URIRef(LNJP[company_record_id])\n",
    "    g.add((Record, RDF.type, LNJP.Record))\n",
    "\n",
    "    # Add Company information as data properties\n",
    "    g.add((Record, LNJP['company_record_id'], Literal(index, datatype=XSD.string)))\n",
    "    g.add((Record, LNJP['employee_count'], Literal(row['employee_count'], datatype=XSD.integer)))\n",
    "    g.add((Record, LNJP['follower_count'], Literal(row['follower_count'], datatype=XSD.integer)))\n",
    "    g.add((Record, LNJP['time_recorded'], Literal(row['time_recorded'], datatype=XSD.dateTimeStamp)))\n",
    "\n",
    "    Company = URIRef(LNJP[\"company_\" + str(row['company_id'])])\n",
    "    g.add((Record, LNJP['IsForCompany'], Company))\n",
    "# Serialize the RDF graph to Turtle format\n",
    "turtle_file_path = 'employee_counts.ttl'\n",
    "\n",
    "# Check if the file already exists\n",
    "if os.path.exists(turtle_file_path):\n",
    "    user_input = input(f\"The file '{turtle_file_path}' already exists. Do you want to rewrite it? (y/n): \").lower()\n",
    "\n",
    "    if user_input != 'y':\n",
    "        print(\"Serialization not saved. Exiting.\")\n",
    "        # You might want to add additional logic or exit the script if the user chooses not to overwrite the file.\n",
    "        exit()\n",
    "\n",
    "# If the file doesn't exist or the user chose to overwrite, proceed with saving the serialization\n",
    "print(\"--- saving serialization ---\")\n",
    "g.serialize(destination=turtle_file_path, format='turtle')\n",
    "\n",
    "print(f\"RDF data exported to {turtle_file_path}\")\n",
    "print(\"Serialization saved successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benefits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CSV files in memory\n",
    "benefits = pd.read_csv(benefitsUrl, sep=',', index_col='benefit_id')\n",
    "\n",
    "#create the graph\n",
    "g = Graph()\n",
    "\n",
    "# Bind the namespaces to a prefix for more readable output\n",
    "g.bind(\"foaf\", FOAF)\n",
    "g.bind(\"xsd\", XSD)\n",
    "g.bind(\"countries\", CNS)\n",
    "g.bind(\"lnjp\", LNJP)\n",
    "g.bind(\"skos\", SKOS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- saving serialization ---\n",
      "RDF data exported to benefits.ttl\n",
      "Serialization saved successfully.\n"
     ]
    }
   ],
   "source": [
    "# Iterate over all rows in the companies DataFrame\n",
    "for index, row in benefits.iterrows():\n",
    "    # Create Company node with a URI\n",
    "    benefit_id = \"benefit_\" + str(index)\n",
    "    Benefit = URIRef(LNJP[benefit_id])\n",
    "    g.add((Benefit, RDF.type, LNJP.Benefit))\n",
    "\n",
    "    # Add Company information as data properties\n",
    "    g.add((Benefit, LNJP['benefit_id'], Literal(index, datatype=XSD.integer)))\n",
    "    g.add((Benefit, LNJP['inferred'], Literal(row['inferred'], datatype=XSD.boolean)))\n",
    "    g.add((Benefit, LNJP['type'], Literal(row['type'], datatype=XSD.string)))\n",
    "\n",
    "    JobPosting = URIRef(LNJP[\"job_\" + str(row['job_id'])])\n",
    "    g.add((Benefit, LNJP['isForJobPosting'], JobPosting))\n",
    "    \n",
    "# Serialize the RDF graph to Turtle format\n",
    "turtle_file_path = 'benefits.ttl'\n",
    "\n",
    "# Check if the file already exists\n",
    "if os.path.exists(turtle_file_path):\n",
    "    user_input = input(f\"The file '{turtle_file_path}' already exists. Do you want to rewrite it? (y/n): \").lower()\n",
    "\n",
    "    if user_input != 'y':\n",
    "        print(\"Serialization not saved. Exiting.\")\n",
    "        # You might want to add additional logic or exit the script if the user chooses not to overwrite the file.\n",
    "        exit()\n",
    "\n",
    "# If the file doesn't exist or the user chose to overwrite, proceed with saving the serialization\n",
    "print(\"--- saving serialization ---\")\n",
    "g.serialize(destination=turtle_file_path, format='turtle')\n",
    "\n",
    "print(f\"RDF data exported to {turtle_file_path}\")\n",
    "print(\"Serialization saved successfully.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specialities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CSV files in memory\n",
    "specialities = pd.read_csv(specialitiesUrl, sep=',', index_col='speciality_id', keep_default_na=False, na_values=['_'])\n",
    "\n",
    "#create the graph\n",
    "g = Graph()\n",
    "\n",
    "# Bind the namespaces to a prefix for more readable output\n",
    "g.bind(\"foaf\", FOAF)\n",
    "g.bind(\"xsd\", XSD)\n",
    "g.bind(\"countries\", CNS)\n",
    "g.bind(\"lnjp\", LNJP)\n",
    "g.bind(\"skos\", SKOS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- saving serialization ---\n",
      "RDF data exported to speciality.ttl\n",
      "Serialization saved successfully.\n"
     ]
    }
   ],
   "source": [
    "# Iterate over all rows in the companies DataFrame\n",
    "for index, row in specialities.iterrows():\n",
    "    # Create Company node with a URI\n",
    "    speciality_id = \"speciality_\" + str(index)\n",
    "    Speciality = URIRef(LNJP[speciality_id])\n",
    "    g.add((Speciality, RDF.type, LNJP.Speciality))\n",
    "\n",
    "    # Add Company information as data properties\n",
    "    g.add((Speciality, LNJP['speciality_id'], Literal(index, datatype=XSD.integer)))\n",
    "    g.add((Speciality, LNJP['speciality'], Literal(row['speciality'], datatype=XSD.string)))\n",
    "# Serialize the RDF graph to Turtle format\n",
    "turtle_file_path = 'speciality.ttl'\n",
    "\n",
    "# Check if the file already exists\n",
    "if os.path.exists(turtle_file_path):\n",
    "    user_input = input(f\"The file '{turtle_file_path}' already exists. Do you want to rewrite it? (y/n): \").lower()\n",
    "\n",
    "    if user_input != 'y':\n",
    "        print(\"Serialization not saved. Exiting.\")\n",
    "        # You might want to add additional logic or exit the script if the user chooses not to overwrite the file.\n",
    "        exit()\n",
    "\n",
    "# If the file doesn't exist or the user chose to overwrite, proceed with saving the serialization\n",
    "print(\"--- saving serialization ---\")\n",
    "g.serialize(destination=turtle_file_path, format='turtle')\n",
    "\n",
    "print(f\"RDF data exported to {turtle_file_path}\")\n",
    "print(\"Serialization saved successfully.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Company_Specialities Join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CSV files in memory\n",
    "company_specialities = pd.read_csv(company_specialitiesUrl, sep=',', index_col='company_id', keep_default_na=False, na_values=['_'])\n",
    "\n",
    "#create the graph\n",
    "g = Graph()\n",
    "\n",
    "# Bind the namespaces to a prefix for more readable output\n",
    "g.bind(\"foaf\", FOAF)\n",
    "g.bind(\"xsd\", XSD)\n",
    "g.bind(\"countries\", CNS)\n",
    "g.bind(\"lnjp\", LNJP)\n",
    "g.bind(\"skos\", SKOS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- saving serialization ---\n",
      "RDF data exported to company_specialities_join.ttl\n",
      "Serialization saved successfully.\n"
     ]
    }
   ],
   "source": [
    "# Assuming you have a DataFrame called 'skillsDataFrame' with columns 'skill_abr' and 'skill_name'\n",
    "# Iterate over all rows in the skills DataFrame\n",
    "for index, row in company_specialities.iterrows():\n",
    "    # Create Skill node with a URI\n",
    "    # skillId = \"skill_\" + str(skill_abr)\n",
    "    \n",
    "    company_id = \"company_\" + str(index)\n",
    "    Company = URIRef(LNJP[company_id])\n",
    "    Speciality = URIRef(LNJP[\"speciality_\" + str(row['speciality_id'])])\n",
    "    g.add((Company, LNJP['hasSpeciality'], Speciality))\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "# Serialize the RDF graph to Turtle format\n",
    "turtle_file_path = 'company_specialities_join.ttl'\n",
    "\n",
    "# Check if the file already exists\n",
    "if os.path.exists(turtle_file_path):\n",
    "    user_input = input(f\"The file '{turtle_file_path}' already exists. Do you want to rewrite it? (y/n): \").lower()\n",
    "\n",
    "    if user_input != 'y':\n",
    "        print(\"Serialization not saved. Exiting.\")\n",
    "        # You might want to add additional logic or exit the script if the user chooses not to overwrite the file.\n",
    "        exit()\n",
    "\n",
    "# If the file doesn't exist or the user chose to overwrite, proceed with saving the serialization\n",
    "print(\"--- saving serialization ---\")\n",
    "g.serialize(destination=turtle_file_path, format='turtle')\n",
    "\n",
    "print(f\"RDF data exported to {turtle_file_path}\")\n",
    "print(\"Serialization saved successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Salaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CSV files in memory\n",
    "salaries = pd.read_csv(salariesUrl, sep=',', index_col='salary_id')\n",
    "\n",
    "#create the graph\n",
    "g = Graph()\n",
    "\n",
    "# Bind the namespaces to a prefix for more readable output\n",
    "g.bind(\"foaf\", FOAF)\n",
    "g.bind(\"xsd\", XSD)\n",
    "g.bind(\"countries\", CNS)\n",
    "g.bind(\"lnjp\", LNJP)\n",
    "g.bind(\"skos\", SKOS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- saving serialization ---\n",
      "RDF data exported to salaries.ttl\n",
      "Serialization saved successfully.\n"
     ]
    }
   ],
   "source": [
    "# Iterate over all rows in the companies DataFrame\n",
    "for index, row in salaries.iterrows():\n",
    "    # Create Company node with a URI\n",
    "    salary_id = \"salary_\" + str(index)\n",
    "    Salary = URIRef(LNJP[salary_id])\n",
    "    g.add((Salary, RDF.type, LNJP.Salary))\n",
    "\n",
    "    # Add Company information as data properties\n",
    "    g.add((Salary, LNJP['salary_id'], Literal(index, datatype=XSD.integer)))\n",
    "    g.add((Salary, LNJP['max_salary'], Literal(row['max_salary'], datatype=XSD.decimal)))\n",
    "    g.add((Salary, LNJP['med_salary'], Literal(row['med_salary'], datatype=XSD.decimal)))\n",
    "    g.add((Salary, LNJP['min_salary'], Literal(row['min_salary'], datatype=XSD.decimal)))\n",
    "    g.add((Salary, LNJP['pay_period'], Literal(row['pay_period'], datatype=XSD.string)))\n",
    "    g.add((Salary, LNJP['currency'], Literal(row['currency'], datatype=XSD.string)))\n",
    "    g.add((Salary, LNJP['compensation_type'], Literal(row['compensation_type'], datatype=XSD.string)))\n",
    "\n",
    "    JobPosting = URIRef(LNJP[\"job_\" + str(row['job_id'])])\n",
    "    g.add((Salary, LNJP['isAllocatedFor'], JobPosting))\n",
    "# Serialize the RDF graph to Turtle format\n",
    "turtle_file_path = 'salaries.ttl'\n",
    "\n",
    "# Check if the file already exists\n",
    "if os.path.exists(turtle_file_path):\n",
    "    user_input = input(f\"The file '{turtle_file_path}' already exists. Do you want to rewrite it? (y/n): \").lower()\n",
    "\n",
    "    if user_input != 'y':\n",
    "        print(\"Serialization not saved. Exiting.\")\n",
    "        # You might want to add additional logic or exit the script if the user chooses not to overwrite the file.\n",
    "        exit()\n",
    "\n",
    "# If the file doesn't exist or the user chose to overwrite, proceed with saving the serialization\n",
    "print(\"--- saving serialization ---\")\n",
    "g.serialize(destination=turtle_file_path, format='turtle')\n",
    "\n",
    "print(f\"RDF data exported to {turtle_file_path}\")\n",
    "print(\"Serialization saved successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EXTRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the country codes\n",
    "# we need to convert NaN values to something else otherwise NA strings are converted to NaN -> problem with Namibia\n",
    "countries = pd.read_csv(countriesURL, sep=',', index_col='name', keep_default_na=False, na_values=['_'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from difflib import SequenceMatcher\n",
    "import numpy as np\n",
    "\n",
    "import unicodedata\n",
    "def strip_accents(s):\n",
    "   return ''.join(c for c in unicodedata.normalize('NFD', s)\n",
    "                  if unicodedata.category(c) != 'Mn')\n",
    "\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'players' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m<timed exec>:4\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'players' is not defined"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "#measure execution time\n",
    "\n",
    "#iterate over the players dataframe\n",
    "for index, row in players.iterrows():\n",
    "    # Create the node to add to the Graph\n",
    "    # the node has the namespace + the player id as URI\n",
    "    idU = \"player\"+str(index)\n",
    "    Player = URIRef(SO[idU])\n",
    "    # the transferMarkt profile has as URI, the URL of the profile in the website\n",
    "    TransfermarktProfile = URIRef(row['url'])\n",
    "    \n",
    "    # Add triples using store's add() method.\n",
    "    g.add((Player, RDF.type, SO.Player))\n",
    "    g.add((TransfermarktProfile, RDF.type, SO.TransfermarktProfile))\n",
    "    g.add((TransfermarktProfile, SO['isAbout'], Player))\n",
    "    \n",
    "    #process player name\n",
    "    name = row['name'].split('-')\n",
    "\n",
    "    if (len(name)>1):\n",
    "        g.add((Player, SO['firstName'], Literal(name[0], datatype=XSD.string)))\n",
    "        g.add((Player, SO['lastName'], Literal(name[1], datatype=XSD.string)))\n",
    "    else:\n",
    "        g.add((Player, SO['lastName'], Literal(name[0], datatype=XSD.string)))\n",
    "        \n",
    "    #there can be more than one position per player\n",
    "    for pos in row['position'].split(' - '):\n",
    "        g.add((Player, SO['position'], Literal(pos.lower(), datatype=XSD.string)))\n",
    "    \n",
    "    if not(row['club_id']==''):\n",
    "        idC = \"club\"+str(row['club_id'])\n",
    "        g.add((Player, SO['playFor'], URIRef(SO[idC])))\n",
    "\n",
    "#iterate over the fifa dataframe\n",
    "for index, row in playersFifa.iterrows():\n",
    "    pname = row['short_name'].lower()\n",
    "    if ('.' in pname):\n",
    "        # get last name\n",
    "        # in the fifa dataset we have short names as L. Messi so we delete the L. \n",
    "        # we need to check if the last name contains a space\n",
    "        pname = row['short_name'].split('.')[1].lower().strip()\n",
    "        if ' ' in pname:\n",
    "            i = 0\n",
    "            for t in pname.split(' '):\n",
    "                if i == 0:\n",
    "                    pname = t.lower()\n",
    "                else:\n",
    "                    pname = pname + \"-\" + t.lower()\n",
    "                i += 1           \n",
    "    elif(' ' in pname):\n",
    "        # here we have to handle Cristiano Ronaldo mapping it to cristiano-ronaldo to maximize the match in the players dataframe \n",
    "        i = 0\n",
    "        for t in row['short_name'].split(' '):\n",
    "            if i == 0:\n",
    "                pname = t.lower()\n",
    "            else:\n",
    "                pname = pname + \"-\" + t.lower()\n",
    "            i += 1\n",
    "    pname = strip_accents(pname)\n",
    "    \n",
    "    # find sim with the full name \n",
    "    fullname = row['long_name'].lower()\n",
    "    i = 0\n",
    "    for t in fullname.split(' '):\n",
    "        if i == 0:\n",
    "            fullname = t.lower()\n",
    "        else:\n",
    "            fullname = fullname + \"-\" + t.lower()\n",
    "        i += 1 \n",
    "    fullname = strip_accents(fullname)\n",
    "    # check the players with that last name\n",
    "    names =  players[players['name'].str.contains(pname)]['name']\n",
    "    #find max similarity    \n",
    "    maxN = 0\n",
    "    playerId = ''\n",
    "    for n in names:\n",
    "        sim = SequenceMatcher(None, fullname, n).ratio()\n",
    "        if (maxN < sim):\n",
    "            maxN = sim\n",
    "            playerId = players.loc[players['name'] == n].index[0]\n",
    "        \n",
    "    #if we get a valid playerId we can connect the Fifa stats to the transfermrkt player\n",
    "    if (playerId != ''):\n",
    "        #remove the row from the player dataframe to avoid futher matchings (we know data will contain errors)\n",
    "        players = players.drop(index=playerId)\n",
    "        idU = \"player\"+str(playerId)\n",
    "        Player = URIRef(SO[idU])\n",
    "        g.add((Player, SO['overallFifaValue'], Literal(row['overall'], datatype=XSD.int)))\n",
    "        g.add((Player, SO['growthFifaPotential'], Literal(row['potential'], datatype=XSD.int)))\n",
    "        g.add((Player, SO['economicValue'], Literal(row['value_eur'], datatype=XSD.int)))\n",
    "        g.add((Player, SO['annualWage'], Literal(row['wage_eur'], datatype=XSD.int))) \n",
    "        \n",
    "        pFeatures = str(row['player_tags'])\n",
    "        if pFeatures != '_' and pFeatures != '':\n",
    "            pFeatures = pFeatures.split(',')\n",
    "            for feature in pFeatures:\n",
    "                feature = feature.strip()\n",
    "                feature = re.sub('#', '', feature)\n",
    "                g.add((Player, SO['playerFeature'], Literal(feature, datatype=XSD.string)))\n",
    "        \n",
    "        if row['contract_valid_until'] != '_' and row['contract_valid_until'] != '':\n",
    "            g.add((Player, SO['contractValidTo'], Literal(int(row['contract_valid_until']), datatype=XSD.gYear)))        \n",
    "\n",
    "        g.add((Player, SO['birthday'], Literal(row['dob'], datatype=XSD.date)))\n",
    "        g.add((Player, SO['height'], Literal(row['height_cm'], datatype=XSD.int)))\n",
    "        g.add((Player, SO['weight'], Literal(row['weight_kg'], datatype=XSD.int)))\n",
    "        \n",
    "        \n",
    "        nationality = row['nationality'] \n",
    "        nationality = nationality.replace(\" \", \"_\")\n",
    "        # create the RDF node\n",
    "        Country = URIRef(CNS[nationality])\n",
    "        # add the edge connecting the Movie and the Country \n",
    "        g.add((Player, SO['nationality'], Country))   \n",
    "\n",
    "        # Homework: extend the code to populate the 'propertyOf' edge\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- saving serialization ---\n",
      "CPU times: user 1.51 s, sys: 8.25 ms, total: 1.52 s\n",
      "Wall time: 1.52 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# print all the data in the Turtle format\n",
    "print(\"--- saving serialization ---\")\n",
    "with open(savePath + 'players.ttl', 'w') as file:\n",
    "    file.write(g.serialize(format='turtle'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CSV files in memory\n",
    "apps = pd.read_csv(appearancesUrl, sep=',', index_col='appearance_id', keep_default_na=False, na_values=['_'])\n",
    "games = pd.read_csv(gamesUrl, sep=',', index_col='game_id', keep_default_na=False, na_values=['_'])\n",
    "\n",
    "#create the graph\n",
    "g = Graph()\n",
    "\n",
    "# Bind the namespaces to a prefix for more readable output\n",
    "g.bind(\"foaf\", FOAF)\n",
    "g.bind(\"xsd\", XSD)\n",
    "g.bind(\"countries\", CNS)\n",
    "g.bind(\"so\", SO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#iterate over the games dataframe\n",
    "for index, row in games.iterrows():\n",
    "    # we use the transfermrket URL as URI\n",
    "    Game = URIRef(row['url'])\n",
    "    g.add((Game, RDF.type, SO.Game))\n",
    "    idU1 = \"club\"+str(row['home_club_id'])\n",
    "    idU2 = \"club\"+str(row['away_club_id'])\n",
    "    HomeClub = URIRef(SO[idU1])\n",
    "    AwayClub = URIRef(SO[idU2])\n",
    "    g.add((Game, SO['homeClub'], HomeClub))\n",
    "    g.add((Game, SO['awayClub'], AwayClub))    \n",
    "    g.add((Game, SO['matchDay'], Literal(row['date'], datatype=XSD.date)))\n",
    "    g.add((Game, SO['homeClubGoals'], Literal(row['home_club_goals'], datatype=XSD.int)))\n",
    "    g.add((Game, SO['awayClubGoals'], Literal(row['away_club_goals'], datatype=XSD.int)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- saving serialization ---\n",
      "CPU times: user 404 ms, sys: 3.27 ms, total: 407 ms\n",
      "Wall time: 409 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# print all the data in the Turtle format\n",
    "print(\"--- saving serialization ---\")\n",
    "with open(savePath + 'games.ttl', 'w') as file:\n",
    "    file.write(g.serialize(format='turtle'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reload the full players dataframe\n",
    "players = pd.read_csv(playersUrl, sep=',', index_col='player_id', keep_default_na=False, na_values=['_'])\n",
    "#create the graph\n",
    "g = Graph()\n",
    "\n",
    "# Bind the namespaces to a prefix for more readable output\n",
    "g.bind(\"foaf\", FOAF)\n",
    "g.bind(\"xsd\", XSD)\n",
    "g.bind(\"countries\", CNS)\n",
    "g.bind(\"so\", SO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oldgameid = ''\n",
    "for index, row in apps.iterrows():\n",
    "    idA = \"appearance\"+str(index)\n",
    "    idP = \"player\"+str(row['player_id'])\n",
    "    Appearance = URIRef(SO[idA])\n",
    "    Player = URIRef(SO[idP])\n",
    "    currgameid = str(row['game_id'])\n",
    "    idG = \"game\"+currgameid\n",
    "    Game = URIRef(SO[idG])\n",
    "    g.add((Appearance, RDF.type, SO.Appearance))\n",
    "    g.add((Player, SO['appearIn'], Appearance))\n",
    "    g.add((Appearance, SO['playIn'], Game))\n",
    "\n",
    "    g.add((Appearance, SO['goals'], Literal(row['goals'], datatype=XSD.int)))\n",
    "    g.add((Appearance, SO['assists'], Literal(row['assists'], datatype=XSD.int)))\n",
    "    g.add((Appearance, SO['minutesPlayed'], Literal(row['minutes_played'], datatype=XSD.int)))\n",
    "    g.add((Appearance, SO['yellowCard'], Literal(row['yellow_cards'], datatype=XSD.int)))\n",
    "    g.add((Appearance, SO['redCard'], Literal(row['red_cards'], datatype=XSD.int)))\n",
    "\n",
    "    #add this triple only once per game\n",
    "    if (currgameid != oldgameid):\n",
    "        idL = \"league\"+str(row['league_id'])\n",
    "        g.add((Game, SO['belongTo'], URIRef(SO[idL])))\n",
    "        oldgameid = currgameid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- saving serialization ---\n",
      "CPU times: user 12.2 s, sys: 48.5 ms, total: 12.3 s\n",
      "Wall time: 12.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# print all the data in the Turtle format\n",
    "print(\"--- saving serialization ---\")\n",
    "with open(savePath + 'appearances.ttl', 'w') as file:\n",
    "    file.write(g.serialize(format='turtle'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
