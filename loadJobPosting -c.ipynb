{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Populate the Job Posting Ontology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# required libraries\n",
    "import pandas as pd\n",
    "import os\n",
    "from pathlib import Path\n",
    "# Load the required libraries\n",
    "from rdflib import Graph, Literal, RDF, URIRef, Namespace, BNode, OWL, RDFS\n",
    "# rdflib knows about some namespaces, like FOAF\n",
    "from rdflib.namespace import FOAF, XSD\n",
    "# CHECK DATE \n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Navid\\Documents\\GitHub\n",
      "data/job_posting.csv\n"
     ]
    }
   ],
   "source": [
    "# parameters and URLs\n",
    "path = str(Path(os.path.abspath(os.getcwd())).parent.absolute())\n",
    "print(path)\n",
    "jobPostingUrl = 'data/job_posting.csv'\n",
    "print(jobPostingUrl)\n",
    "\n",
    "companiesUrl = path + '/GraphDB/data/company_details/companies.csv'\n",
    "company_industriesUrl = path + '/GraphDB/data/company_details/company_industries.csv'\n",
    "company_specialitiesUrl = path + '/GraphDB/data/company_details/company_specialities.csv'\n",
    "specialitiesUrl = path + '/GraphDB/data/company_details/specialities.csv'\n",
    "employee_countsUrl = path + '/GraphDB/data/company_details/employee_counts.csv'\n",
    "c = path + '/GraphDB/data/company_details/specialities.csv'\n",
    "\n",
    "benefitsUrl = path + '/GraphDB/data/job_details/benefits.csv'\n",
    "job_industriesUrl = path + '/GraphDB/data/job_details/job_industries.csv'\n",
    "job_skillsUrl = path + '/GraphDB/data/job_details/job_skills.csv'\n",
    "salariesUrl = path + '/GraphDB/data/job_details/salaries.csv'\n",
    "\n",
    "industriesUrl = path + '/GraphDB/data/maps/industries.csv'\n",
    "skillsUrl = path + '/GraphDB/data/maps/skills.csv'\n",
    "\n",
    "\n",
    "# country codes\n",
    "countriesURL = path + '/GraphDB/data/countries/all.csv'\n",
    "\n",
    "# saving folder\n",
    "savePath =  path + '/data/rdf/linkedinDB/' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://eulersharp.sourceforge.net/2003/03swap/countries#\n"
     ]
    }
   ],
   "source": [
    "# Construct the country and the movie ontology namespaces not known by RDFlib\n",
    "CNS = Namespace(\"http://eulersharp.sourceforge.net/2003/03swap/countries#\")\n",
    "LNJP = Namespace(\"http://www.dei.unipd.it/database2/LinkedinJobPosting#\")\n",
    "SKOS = Namespace(\"https://www.w3.org/2009/08/skos-reference/skos-owl1-dl.rdf\")\n",
    "print(CNS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Job Posting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CSV files in memory\n",
    "jobPosting = pd.read_csv(jobPostingUrl, sep=',', index_col='job_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create the graph\n",
    "g = Graph()\n",
    "\n",
    "# Bind the namespaces to a prefix for more readable output\n",
    "g.bind(\"foaf\", FOAF)\n",
    "g.bind(\"xsd\", XSD)\n",
    "g.bind(\"countries\", CNS)\n",
    "g.bind(\"lnjp\", LNJP)\n",
    "g.bind(\"skos\", SKOS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- saving serialization ---\n",
      "RDF data exported to job_postings.ttl\n",
      "Serialization saved successfully.\n"
     ]
    }
   ],
   "source": [
    "# Iterate over the job postings DataFrame\n",
    "for index, row in jobPosting.iterrows():\n",
    "    # Create the JobPosting node with a URI\n",
    "    jobPostingId = \"job_\" + str(index)\n",
    "    JobPosting = URIRef(LNJP[jobPostingId])\n",
    "    g.add((JobPosting, RDF.type, LNJP.JobPosting))\n",
    "\n",
    "    # Add data properties\n",
    "    g.add((JobPosting, LNJP.job_id, Literal(index, datatype=XSD.integer)))\n",
    "    g.add((JobPosting, LNJP.title, Literal(row['title'], datatype=XSD.string)))\n",
    "    g.add((JobPosting, LNJP.description, Literal(row['description'], datatype=XSD.string)))\n",
    "    if not pd.isnull(row['max_salary']) and row['max_salary'] != \"\":\n",
    "        g.add((JobPosting, LNJP.max_salary, Literal(row['max_salary'], datatype=XSD.decimal)))\n",
    "    if not pd.isnull(row['med_salary']) and row['med_salary'] != \"\":\n",
    "        g.add((JobPosting, LNJP.med_salary, Literal(row['med_salary'], datatype=XSD.decimal)))\n",
    "    if not pd.isnull(row['min_salary']) and row['min_salary'] != \"\":\n",
    "        g.add((JobPosting, LNJP.min_salary, Literal(row['min_salary'], datatype=XSD.decimal)))\n",
    "    if not pd.isnull(row['pay_period']) and row['pay_period'] != \"\":    \n",
    "        g.add((JobPosting, LNJP.pay_period, Literal(row['pay_period'], datatype=XSD.string)))\n",
    "    g.add((JobPosting, LNJP.location, Literal(row['location'], datatype=XSD.string)))\n",
    "    if pd.notnull(row['applies']) and str(row['applies']).strip():\n",
    "        g.add((JobPosting, LNJP.applies, Literal(int(row['applies']), datatype=XSD.integer)))\n",
    "    g.add((JobPosting, LNJP.original_listed_time, Literal(row['original_listed_time'], datatype=XSD.date)))\n",
    "    if not pd.isnull(row['remote_allowed']) and row['remote_allowed'] != \"\":\n",
    "        g.add((JobPosting, LNJP.remote_allowed, Literal(row['remote_allowed'], datatype=XSD.integer)))\n",
    "    if not pd.isnull(row['views']) and row['views'] != \"\":    \n",
    "        g.add((JobPosting, LNJP.views, Literal(int(row['views']), datatype=XSD.integer)))\n",
    "    g.add((JobPosting, LNJP.job_posting_url, Literal(row['job_posting_url'], datatype=XSD.string)))\n",
    "    if not pd.isnull(row['application_url']) and row['application_url'] != \"\":  \n",
    "        g.add((JobPosting, LNJP.application_url, Literal(row['application_url'], datatype=XSD.string)))\n",
    "    g.add((JobPosting, LNJP.application_type, Literal(row['application_type'], datatype=XSD.string)))\n",
    "    g.add((JobPosting, LNJP.expiry, Literal(row['expiry'], datatype=XSD.date)))\n",
    "    if not pd.isnull(row['closed_time']) and row['closed_time'] != \"\":\n",
    "        g.add((JobPosting, LNJP.closed_time, Literal(row['closed_time'], datatype=XSD.date)))\n",
    "    if not pd.isnull(row['formatted_experience_level']) and row['formatted_experience_level'] != \"\":    \n",
    "        g.add((JobPosting, LNJP.formatted_experience_level, Literal(row['formatted_experience_level'], datatype=XSD.string)))\n",
    "    if not pd.isnull(row['skills_desc']) and row['skills_desc'] != \"\":\n",
    "        g.add((JobPosting, LNJP.skills_desc, Literal(row['skills_desc'], datatype=XSD.string)))\n",
    "    g.add((JobPosting, LNJP.listed_time, Literal(row['listed_time'], datatype=XSD.date)))\n",
    "    if not pd.isnull(row['posting_domain']) and row['posting_domain'] != \"\":\n",
    "        g.add((JobPosting, LNJP.posting_domain, Literal(row['posting_domain'], datatype=XSD.string)))\n",
    "    g.add((JobPosting, LNJP.sponsored, Literal(row['sponsored'], datatype=XSD.integer)))\n",
    "    g.add((JobPosting, LNJP.work_type, Literal(row['work_type'], datatype=XSD.string)))\n",
    "    if not pd.isnull(row['currency']) and row['currency'] != \"\":\n",
    "        g.add((JobPosting, LNJP.currency, Literal(row['currency'], datatype=XSD.string)))\n",
    "    if not pd.isnull(row['compensation_type']) and row['compensation_type'] != \"\":\n",
    "        g.add((JobPosting, LNJP.compensation_type, Literal(row['compensation_type'], datatype=XSD.string)))\n",
    "\n",
    "    if pd.notnull(row['company_id']) and str(row['company_id']).strip():  # Check for NaN and empty string\n",
    "        Company = URIRef(LNJP[\"company_\" + str(int(row['company_id']))])\n",
    "        if Company:  # Checking if Company is not empty\n",
    "            g.add((JobPosting, LNJP['hasCompany'], Company))\n",
    "# terminated\n",
    "# jobSkillsAbr = \"skill_\" + str(row['abr']) # You should replace this with the actual ID or URI\n",
    "# JobSkills = URIRef(LNJP[jobSkillsAbr])\n",
    "# print(JobSkills)\n",
    "\n",
    "# Serialize the RDF graph to Turtle format\n",
    "turtle_file_path = 'job_postings.ttl'\n",
    "\n",
    "# Check if the file already exists\n",
    "if os.path.exists(turtle_file_path):\n",
    "    user_input = input(f\"The file '{turtle_file_path}' already exists. Do you want to rewrite it? (y/n): \").lower()\n",
    "    \n",
    "    if user_input != 'y':\n",
    "        print(\"Serialization not saved. Exiting.\")\n",
    "        # You might want to add additional logic or exit the script if the user chooses not to overwrite the file.\n",
    "        exit()\n",
    "\n",
    "# If the file doesn't exist or the user chose to overwrite, proceed with saving the serialization\n",
    "print(\"--- saving serialization ---\")\n",
    "g.serialize(destination=turtle_file_path, format='turtle')\n",
    "\n",
    "print(f\"RDF data exported to {turtle_file_path}\")\n",
    "print(\"Serialization saved successfully.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Skills"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skills DataFrame read successfully.\n"
     ]
    }
   ],
   "source": [
    "# Load the CSV files in memory\n",
    "skills = pd.read_csv(skillsUrl, sep=',', index_col='skill_abr', keep_default_na=False, na_values=['_'])\n",
    "assert not skills.empty, \"Skills DataFrame is empty. Check the CSV file or URL.\"\n",
    "\n",
    "# Print a message if the test passes\n",
    "print(\"Skills DataFrame read successfully.\")\n",
    "#create the graph\n",
    "g = Graph()\n",
    "\n",
    "# Bind the namespaces to a prefix for more readable output\n",
    "g.bind(\"foaf\", FOAF)\n",
    "g.bind(\"xsd\", XSD)\n",
    "g.bind(\"countries\", CNS)\n",
    "g.bind(\"lnjp\", LNJP)\n",
    "g.bind(\"skos\", SKOS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- saving serialization ---\n",
      "RDF data exported to skills.ttl\n",
      "Serialization saved successfully.\n"
     ]
    }
   ],
   "source": [
    "# Assuming you have a DataFrame called 'skillsDataFrame' with columns 'skill_abr' and 'skill_name'\n",
    "# Iterate over all rows in the skills DataFrame\n",
    "for skill_abr, row in skills.iterrows():\n",
    "    # Create Skill node with a URI\n",
    "    skillId = \"skill_\" + str(skill_abr)\n",
    "    Skill = URIRef(LNJP[skillId])\n",
    "    g.add((Skill, RDF.type, SKOS.Concept))\n",
    "    g.add((Skill, RDF.type, LNJP.Skill))\n",
    "\n",
    "    # Add skill information as data properties\n",
    "    g.add((Skill, LNJP.skill_abr, Literal(skill_abr, datatype=XSD.string)))\n",
    "    g.add((Skill, LNJP.skill_name, Literal(row['skill_name'], datatype=XSD.string)))\n",
    "\n",
    "    if(row['broader_concept']):\n",
    "        broaderSkillId = \"skill_\" + str(row['broader_concept'])\n",
    "        broaderSkill = URIRef(LNJP[broaderSkillId])\n",
    "        g.add((Skill, SKOS.broader, broaderSkill))\n",
    "    \n",
    "    if(row['narrower_concept']):\n",
    "        narrowerSkillId = \"skill_\" + str(row['narrower_concept'])\n",
    "        narrowerSkill = URIRef(LNJP[narrowerSkillId])\n",
    "        g.add((Skill, SKOS.narrower, narrowerSkill))\n",
    "        \n",
    "    if(row['related_concept']):\n",
    "        relatedSkillId = \"skill_\" + str(row['related_concept'])\n",
    "        relatedSkill = URIRef(LNJP[relatedSkillId])\n",
    "        g.add((Skill, SKOS.related, relatedSkill))\n",
    "    \n",
    "\n",
    "# Serialize the RDF graph to Turtle format\n",
    "turtle_file_path = 'skills.ttl'\n",
    "\n",
    "# Check if the file already exists\n",
    "if os.path.exists(turtle_file_path):\n",
    "    user_input = input(f\"The file '{turtle_file_path}' already exists. Do you want to rewrite it? (y/n): \").lower()\n",
    "\n",
    "    if user_input != 'y':\n",
    "        print(\"Serialization not saved. Exiting.\")\n",
    "        # You might want to add additional logic or exit the script if the user chooses not to overwrite the file.\n",
    "        exit()\n",
    "\n",
    "# If the file doesn't exist or the user chose to overwrite, proceed with saving the serialization\n",
    "print(\"--- saving serialization ---\")\n",
    "g.serialize(destination=turtle_file_path, format='turtle')\n",
    "\n",
    "print(f\"RDF data exported to {turtle_file_path}\")\n",
    "print(\"Serialization saved successfully.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## JOB_SKILLS JOIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skills DataFrame read successfully.\n"
     ]
    }
   ],
   "source": [
    "# Load the CSV files in memory\n",
    "job_skills = pd.read_csv(job_skillsUrl, sep=',', index_col='job_id')\n",
    "assert not job_skills.empty, \"Skills DataFrame is empty. Check the CSV file or URL.\"\n",
    "\n",
    "# Print a message if the test passes\n",
    "print(\"Skills DataFrame read successfully.\")\n",
    "#create the graph\n",
    "g = Graph()\n",
    "\n",
    "# Bind the namespaces to a prefix for more readable output\n",
    "g.bind(\"foaf\", FOAF)\n",
    "g.bind(\"xsd\", XSD)\n",
    "g.bind(\"countries\", CNS)\n",
    "g.bind(\"lnjp\", LNJP)\n",
    "g.bind(\"skos\", SKOS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- saving serialization ---\n",
      "RDF data exported to job_skills_join.ttl\n",
      "Serialization saved successfully.\n"
     ]
    }
   ],
   "source": [
    "# Assuming you have a DataFrame called 'skillsDataFrame' with columns 'skill_abr' and 'skill_name'\n",
    "# Iterate over all rows in the skills DataFrame\n",
    "for index, row in job_skills.iterrows():\n",
    "    # Create Skill node with a URI\n",
    "    # skillId = \"skill_\" + str(skill_abr)\n",
    "    \n",
    "    jobPostingId = \"job_\" + str(index)\n",
    "    JobPosting = URIRef(LNJP[jobPostingId])\n",
    "    skillId = \"skill_\" + str(row['skill_abr'])\n",
    "    Skill = URIRef(LNJP[skillId])\n",
    "    g.add((JobPosting, LNJP['hasSkill'], Skill))\n",
    "    \n",
    "# Serialize the RDF graph to Turtle format\n",
    "turtle_file_path = 'job_skills_join.ttl'\n",
    "\n",
    "# Check if the file already exists\n",
    "if os.path.exists(turtle_file_path):\n",
    "    user_input = input(f\"The file '{turtle_file_path}' already exists. Do you want to rewrite it? (y/n): \").lower()\n",
    "\n",
    "    if user_input != 'y':\n",
    "        print(\"Serialization not saved. Exiting.\")\n",
    "        # You might want to add additional logic or exit the script if the user chooses not to overwrite the file.\n",
    "        exit()\n",
    "\n",
    "# If the file doesn't exist or the user chose to overwrite, proceed with saving the serialization\n",
    "print(\"--- saving serialization ---\")\n",
    "g.serialize(destination=turtle_file_path, format='turtle')\n",
    "\n",
    "print(f\"RDF data exported to {turtle_file_path}\")\n",
    "print(\"Serialization saved successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Industry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Industries DataFrame read successfully.\n"
     ]
    }
   ],
   "source": [
    "# Load the CSV files in memory\n",
    "industries = pd.read_csv(industriesUrl, sep=',', index_col='industry_id', keep_default_na=False, na_values=['_'])\n",
    "assert not industries.empty, \"Industries DataFrame is empty. Check the CSV file or URL.\"\n",
    "\n",
    "# Print a message if the test passes\n",
    "print(\"Industries DataFrame read successfully.\")\n",
    "#create the graph\n",
    "g = Graph()\n",
    "\n",
    "# Bind the namespaces to a prefix for more readable output\n",
    "g.bind(\"foaf\", FOAF)\n",
    "g.bind(\"xsd\", XSD)\n",
    "g.bind(\"countries\", CNS)\n",
    "g.bind(\"lnjp\", LNJP)\n",
    "g.bind(\"skos\", SKOS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- saving serialization ---\n",
      "RDF data exported to industries.ttl\n",
      "Serialization saved successfully.\n"
     ]
    }
   ],
   "source": [
    "# Assuming you have a DataFrame called 'industiesDataFrom' with columns 'industry_id' and 'industry_name'\n",
    "# Iterate over all rows in the industries DataFrame\n",
    "for index, row in industries.iterrows():\n",
    "    # Create Skill node with a URI\n",
    "    industryId = \"industry_\" + str(index)\n",
    "    Industry = URIRef(LNJP[industryId])\n",
    "    g.add((Industry, RDF.type, LNJP.Industry))\n",
    "\n",
    "    # Add skill information as data properties\n",
    "    g.add((Industry, LNJP['industry_id'], Literal(index, datatype=XSD.integer)))\n",
    "    if not pd.isnull(row['industry_name']) and row['industry_name'] != \"\":\n",
    "        g.add((Industry, LNJP['industry_name'], Literal(row['industry_name'], datatype=XSD.string)))\n",
    "    \n",
    "# Serialize the RDF graph to Turtle format\n",
    "turtle_file_path = 'industries.ttl'\n",
    "\n",
    "# Check if the file already exists\n",
    "if os.path.exists(turtle_file_path):\n",
    "    user_input = input(f\"The file '{turtle_file_path}' already exists. Do you want to rewrite it? (y/n): \").lower()\n",
    "\n",
    "    if user_input != 'y':\n",
    "        print(\"Serialization not saved. Exiting.\")\n",
    "        # You might want to add additional logic or exit the script if the user chooses not to overwrite the file.\n",
    "        exit()\n",
    "\n",
    "# If the file doesn't exist or the user chose to overwrite, proceed with saving the serialization\n",
    "print(\"--- saving serialization ---\")\n",
    "g.serialize(destination=turtle_file_path, format='turtle')\n",
    "\n",
    "print(f\"RDF data exported to {turtle_file_path}\")\n",
    "print(\"Serialization saved successfully.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Job_Industries join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skills DataFrame read successfully.\n"
     ]
    }
   ],
   "source": [
    "# Load the CSV files in memory\n",
    "job_industries = pd.read_csv(job_industriesUrl, sep=',', index_col='job_id')\n",
    "assert not job_industries.empty, \"Skills DataFrame is empty. Check the CSV file or URL.\"\n",
    "\n",
    "# Print a message if the test passes\n",
    "print(\"Skills DataFrame read successfully.\")\n",
    "#create the graph\n",
    "g = Graph()\n",
    "\n",
    "# Bind the namespaces to a prefix for more readable output\n",
    "g.bind(\"foaf\", FOAF)\n",
    "g.bind(\"xsd\", XSD)\n",
    "g.bind(\"countries\", CNS)\n",
    "g.bind(\"lnjp\", LNJP)\n",
    "g.bind(\"skos\", SKOS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- saving serialization ---\n",
      "RDF data exported to job_industries_join.ttl\n",
      "Serialization saved successfully.\n"
     ]
    }
   ],
   "source": [
    "# Assuming you have a DataFrame called 'skillsDataFrame' with columns 'skill_abr' and 'skill_name'\n",
    "# Iterate over all rows in the skills DataFrame\n",
    "for index, row in job_industries.iterrows():\n",
    "    # Create Skill node with a URI\n",
    "    # skillId = \"skill_\" + str(skill_abr)\n",
    "    \n",
    "    jobPosting_id = \"job_\" + str(index)\n",
    "    JobPosting = URIRef(LNJP[jobPosting_id])\n",
    "    Industry = URIRef(LNJP[\"industry_\" + str(row['industry_id'])])\n",
    "    g.add((JobPosting, LNJP['hasIndustryType'], Industry))\n",
    "    \n",
    "# Serialize the RDF graph to Turtle format\n",
    "turtle_file_path = 'job_industries_join.ttl'\n",
    "\n",
    "# Check if the file already exists\n",
    "if os.path.exists(turtle_file_path):\n",
    "    user_input = input(f\"The file '{turtle_file_path}' already exists. Do you want to rewrite it? (y/n): \").lower()\n",
    "\n",
    "    if user_input != 'y':\n",
    "        print(\"Serialization not saved. Exiting.\")\n",
    "        # You might want to add additional logic or exit the script if the user chooses not to overwrite the file.\n",
    "        exit()\n",
    "\n",
    "# If the file doesn't exist or the user chose to overwrite, proceed with saving the serialization\n",
    "print(\"--- saving serialization ---\")\n",
    "g.serialize(destination=turtle_file_path, format='turtle')\n",
    "\n",
    "print(f\"RDF data exported to {turtle_file_path}\")\n",
    "print(\"Serialization saved successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Company"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Companies DataFrame read successfully.\n"
     ]
    }
   ],
   "source": [
    "# Load the CSV files in memory\n",
    "companies = pd.read_csv(companiesUrl, sep=',', index_col='company_id', keep_default_na=False, na_values=['_'])\n",
    "assert not companies.empty, \"Companies DataFrame is empty. Check the CSV file or URL.\"\n",
    "\n",
    "# Print a message if the test passes\n",
    "print(\"Companies DataFrame read successfully.\")\n",
    "#create the graph\n",
    "g = Graph()\n",
    "\n",
    "# Bind the namespaces to a prefix for more readable output\n",
    "g.bind(\"foaf\", FOAF)\n",
    "g.bind(\"xsd\", XSD)\n",
    "g.bind(\"countries\", CNS)\n",
    "g.bind(\"lnjp\", LNJP)\n",
    "g.bind(\"skos\", SKOS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- saving serialization ---\n",
      "RDF data exported to companies.ttl\n",
      "Serialization saved successfully.\n"
     ]
    }
   ],
   "source": [
    "largeCompany = LNJP.largeCompany\n",
    "company_size_property = LNJP['company_size']\n",
    "\n",
    "BNL0 = BNode()\n",
    "BNL1 = BNode()\n",
    "BNL2 = BNode()\n",
    "BNL3 = BNode()\n",
    "BNL4 = BNode()\n",
    "BNL5 = BNode()\n",
    "BNL6 = BNode()\n",
    "BNL7 = BNode()\n",
    "BNL8 = BNode()\n",
    "\n",
    "g.add((largeCompany, RDF.type, OWL.Class))\n",
    "g.add((largeCompany, RDFS.subClassOf, LNJP.Company))\n",
    "\n",
    "g.add((largeCompany, OWL.equivalentClass, BNL0))\n",
    "\n",
    "g.add((BNL0, OWL.intersectionOf, BNL1))\n",
    "\n",
    "g.add((BNL1, RDF.type, RDF.List))\n",
    "g.add((BNL1, RDF.first, LNJP.Company))\n",
    "g.add((BNL1, RDF.rest, BNL2))\n",
    "\n",
    "g.add((BNL2, RDF.type, RDF.List))\n",
    "g.add((BNL2, RDF.first, BNL3))\n",
    "g.add((BNL2, RDF.rest, RDF.nil))\n",
    "\n",
    "g.add((BNL3, RDF.type, OWL.Restriction))\n",
    "g.add((BNL3, OWL.onProperty, company_size_property))\n",
    "g.add((BNL3, OWL.someValuesFrom, BNL4))\n",
    "\n",
    "g.add((BNL4, RDF.type, RDFS.Datatype))\n",
    "g.add((BNL4, OWL.onDatatype, XSD.integer))\n",
    "g.add((BNL4, OWL.withRestrictions, BNL5))\n",
    "\n",
    "g.add((BNL5, RDF.type, RDF.List))\n",
    "g.add((BNL6, RDF.type, RDF.List))\n",
    "g.add((BNL7, RDF.type, RDF.List))\n",
    "g.add((BNL8, RDF.type, RDF.List))\n",
    "\n",
    "g.add((BNL5, RDF.first, BNL6))\n",
    "g.add((BNL5, RDF.rest, BNL7))\n",
    "\n",
    "g.add((BNL7, RDF.first, BNL8))\n",
    "g.add((BNL7, RDF.rest, RDF.nil))\n",
    "\n",
    "g.add((BNL6, XSD.minInclusive, Literal(\"6\", datatype=XSD.integer)))\n",
    "g.add((BNL8, XSD.maxInclusive, Literal(\"7\", datatype=XSD.integer)))\n",
    "\n",
    "mediumCompany = LNJP.mediumCompany\n",
    "company_size_property = LNJP['company_size']\n",
    "\n",
    "BNM0 = BNode()\n",
    "BNM1 = BNode()\n",
    "BNM2 = BNode()\n",
    "BNM3 = BNode()\n",
    "BNM4 = BNode()\n",
    "BNM5 = BNode()\n",
    "BNM6 = BNode()\n",
    "BNM7 = BNode()\n",
    "BNM8 = BNode()\n",
    "\n",
    "g.add((mediumCompany, RDF.type, OWL.Class))\n",
    "g.add((mediumCompany, RDFS.subClassOf, LNJP.Company))\n",
    "\n",
    "g.add((mediumCompany, OWL.equivalentClass, BNM0))\n",
    "\n",
    "g.add((BNM0, OWL.intersectionOf, BNM1))\n",
    "\n",
    "g.add((BNM1, RDF.type, RDF.List))\n",
    "g.add((BNM1, RDF.first, LNJP.Company))\n",
    "g.add((BNM1, RDF.rest, BNM2))\n",
    "\n",
    "g.add((BNM2, RDF.type, RDF.List))\n",
    "g.add((BNM2, RDF.first, BNM3))\n",
    "g.add((BNM2, RDF.rest, RDF.nil))\n",
    "\n",
    "g.add((BNM3, RDF.type, OWL.Restriction))\n",
    "g.add((BNM3, OWL.onProperty, company_size_property))\n",
    "g.add((BNM3, OWL.someValuesFrom, BNM4))\n",
    "\n",
    "g.add((BNM4, RDF.type, RDFS.Datatype))\n",
    "g.add((BNM4, OWL.onDatatype, XSD.integer))\n",
    "g.add((BNM4, OWL.withRestrictions, BNM5))\n",
    "\n",
    "g.add((BNM5, RDF.type, RDF.List))\n",
    "g.add((BNM6, RDF.type, RDF.List))\n",
    "g.add((BNM7, RDF.type, RDF.List))\n",
    "g.add((BNM8, RDF.type, RDF.List))\n",
    "\n",
    "g.add((BNM5, RDF.first, BNM6))\n",
    "g.add((BNM5, RDF.rest, BNM7))\n",
    "\n",
    "g.add((BNM7, RDF.first, BNM8))\n",
    "g.add((BNM7, RDF.rest, RDF.nil))\n",
    "\n",
    "g.add((BNM6, XSD.minInclusive, Literal(\"3\", datatype=XSD.integer)))\n",
    "g.add((BNM8, XSD.maxInclusive, Literal(\"5\", datatype=XSD.integer)))\n",
    "\n",
    "smallCompany = LNJP.smallCompany\n",
    "company_size_property = LNJP['company_size']\n",
    "\n",
    "BNS0 = BNode()\n",
    "BNS1 = BNode()\n",
    "BNS2 = BNode()\n",
    "BNS3 = BNode()\n",
    "BNS4 = BNode()\n",
    "BNS5 = BNode()\n",
    "BNS6 = BNode()\n",
    "BNS7 = BNode()\n",
    "BNS8 = BNode()\n",
    "\n",
    "g.add((smallCompany, RDF.type, OWL.Class))\n",
    "g.add((smallCompany, RDFS.subClassOf, LNJP.Company))\n",
    "\n",
    "g.add((smallCompany, OWL.equivalentClass, BNS0))\n",
    "\n",
    "g.add((BNS0, OWL.intersectionOf, BNS1))\n",
    "\n",
    "g.add((BNS1, RDF.type, RDF.List))\n",
    "g.add((BNS1, RDF.first, LNJP.Company))\n",
    "g.add((BNS1, RDF.rest, BNS2))\n",
    "\n",
    "g.add((BNS2, RDF.type, RDF.List))\n",
    "g.add((BNS2, RDF.first, BNS3))\n",
    "g.add((BNS2, RDF.rest, RDF.nil))\n",
    "\n",
    "g.add((BNS3, RDF.type, OWL.Restriction))\n",
    "g.add((BNS3, OWL.onProperty, company_size_property))\n",
    "g.add((BNS3, OWL.someValuesFrom, BNS4))\n",
    "\n",
    "g.add((BNS4, RDF.type, RDFS.Datatype))\n",
    "g.add((BNS4, OWL.onDatatype, XSD.integer))\n",
    "g.add((BNS4, OWL.withRestrictions, BNS5))\n",
    "\n",
    "g.add((BNS5, RDF.type, RDF.List))\n",
    "g.add((BNS6, RDF.type, RDF.List))\n",
    "g.add((BNS7, RDF.type, RDF.List))\n",
    "g.add((BNS8, RDF.type, RDF.List))\n",
    "\n",
    "g.add((BNS5, RDF.first, BNS6))\n",
    "g.add((BNS5, RDF.rest, BNS7))\n",
    "\n",
    "g.add((BNS7, RDF.first, BNS8))\n",
    "g.add((BNS7, RDF.rest, RDF.nil))\n",
    "\n",
    "g.add((BNS6, XSD.minInclusive, Literal(\"1\", datatype=XSD.integer)))\n",
    "g.add((BNS8, XSD.maxInclusive, Literal(\"2\", datatype=XSD.integer)))\n",
    "\n",
    "# Iterate over all rows in the companies DataFrame\n",
    "for index, row in companies.iterrows():\n",
    "    # Create Company node with a URI\n",
    "    companyId = \"company_\" + str(index)\n",
    "    Company = URIRef(LNJP[companyId])\n",
    "    g.add((Company, RDF.type, LNJP.Company))\n",
    "\n",
    "    # Add Company information as data properties\n",
    "    g.add((Company, LNJP['company_id'], Literal(int(index), datatype=XSD.integer)))\n",
    "    g.add((Company, LNJP['name'], Literal(row['name'], datatype=XSD.string)))\n",
    "    if not pd.isnull(row['description']) and row['description'] != \"\":\n",
    "        g.add((Company, LNJP['description'], Literal(row['description'], datatype=XSD.string)))\n",
    "    if not pd.isnull(row['company_size']) and row['company_size'] != \"\":\n",
    "        g.add((Company, LNJP['company_size'], Literal(row['company_size'], datatype=XSD.integer)))\n",
    "        if (Literal(row['company_size'], datatype=XSD.integer) >= Literal(\"6\", datatype=XSD.integer)) and (Literal(row['company_size'], datatype=XSD.integer) <= Literal(\"7\", datatype=XSD.integer)):\n",
    "            g.add((Company,RDF.type,LNJP.largeCompany))\n",
    "        if (Literal(row['company_size'], datatype=XSD.integer) >= Literal(\"3\", datatype=XSD.integer)) and (Literal(row['company_size'], datatype=XSD.integer) <= Literal(\"5\", datatype=XSD.integer)):\n",
    "            g.add((Company,RDF.type,LNJP.mediumCompany))\n",
    "        if (Literal(row['company_size'], datatype=XSD.integer) >= Literal(\"1\", datatype=XSD.integer)) and (Literal(row['company_size'], datatype=XSD.integer) <= Literal(\"2\", datatype=XSD.integer)):\n",
    "            g.add((Company,RDF.type,LNJP.smallCompany))\n",
    "    if not pd.isnull(row['state']) and row['state'] != \"\":    \n",
    "        g.add((Company, LNJP['state'], Literal(row['state'], datatype=XSD.string)))\n",
    "    g.add((Company, LNJP['city'], Literal(row['city'], datatype=XSD.string)))\n",
    "    if not pd.isnull(row['zip_code']) and row['zip_code'] != \"\":   \n",
    "        g.add((Company, LNJP['zip_code'], Literal(row['zip_code'], datatype=XSD.string)))\n",
    "    if not pd.isnull(row['address']) and row['address'] != \"\":    \n",
    "        g.add((Company, LNJP['address'], Literal(row['address'], datatype=XSD.string)))\n",
    "    g.add((Company, LNJP['url'], Literal(row['url'], datatype=XSD.string)))\n",
    "\n",
    "    Country = URIRef(CNS[row['country']])\n",
    "    g.add((Company, LNJP['hasCountry'], Country))\n",
    "# Serialize the RDF graph to Turtle format\n",
    "turtle_file_path = 'companies.ttl'\n",
    "\n",
    "# Check if the file already exists\n",
    "if os.path.exists(turtle_file_path):\n",
    "    user_input = input(f\"The file '{turtle_file_path}' already exists. Do you want to rewrite it? (y/n): \").lower()\n",
    "\n",
    "    if user_input != 'y':\n",
    "        print(\"Serialization not saved. Exiting.\")\n",
    "        # You might want to add additional logic or exit the script if the user chooses not to overwrite the file.\n",
    "        exit()\n",
    "\n",
    "# If the file doesn't exist or the user chose to overwrite, proceed with saving the serialization\n",
    "print(\"--- saving serialization ---\")\n",
    "g.serialize(destination=turtle_file_path, format='turtle')\n",
    "\n",
    "print(f\"RDF data exported to {turtle_file_path}\")\n",
    "print(\"Serialization saved successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Company_Industries join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skills DataFrame read successfully.\n"
     ]
    }
   ],
   "source": [
    "# Load the CSV files in memory\n",
    "company_industries = pd.read_csv(company_industriesUrl, sep=',', index_col='company_id', keep_default_na=False, na_values=['_'])\n",
    "assert not company_industries.empty, \"Skills DataFrame is empty. Check the CSV file or URL.\"\n",
    "\n",
    "# Print a message if the test passes\n",
    "print(\"Skills DataFrame read successfully.\")\n",
    "#create the graph\n",
    "g = Graph()\n",
    "\n",
    "# Bind the namespaces to a prefix for more readable output\n",
    "g.bind(\"foaf\", FOAF)\n",
    "g.bind(\"xsd\", XSD)\n",
    "g.bind(\"countries\", CNS)\n",
    "g.bind(\"lnjp\", LNJP)\n",
    "g.bind(\"skos\", SKOS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- saving serialization ---\n",
      "RDF data exported to company_industries_join.ttl\n",
      "Serialization saved successfully.\n"
     ]
    }
   ],
   "source": [
    "# Assuming you have a DataFrame called 'skillsDataFrame' with columns 'skill_abr' and 'skill_name'\n",
    "# Iterate over all rows in the skills DataFrame\n",
    "for index, row in company_industries.iterrows():\n",
    "    # Create Skill node with a URI\n",
    "    # skillId = \"skill_\" + str(skill_abr)\n",
    "    \n",
    "    company_id = \"company_\" + str(index)\n",
    "    Company = URIRef(LNJP[company_id])\n",
    "    Industry = URIRef(LNJP[\"industry_\" + str(row['industry_id'])])\n",
    "    g.add((Company, LNJP['hasIndustry'], Industry))\n",
    "    \n",
    "# Serialize the RDF graph to Turtle format\n",
    "turtle_file_path = 'company_industries_join.ttl'\n",
    "\n",
    "# Check if the file already exists\n",
    "if os.path.exists(turtle_file_path):\n",
    "    user_input = input(f\"The file '{turtle_file_path}' already exists. Do you want to rewrite it? (y/n): \").lower()\n",
    "\n",
    "    if user_input != 'y':\n",
    "        print(\"Serialization not saved. Exiting.\")\n",
    "        # You might want to add additional logic or exit the script if the user chooses not to overwrite the file.\n",
    "        exit()\n",
    "\n",
    "# If the file doesn't exist or the user chose to overwrite, proceed with saving the serialization\n",
    "print(\"--- saving serialization ---\")\n",
    "g.serialize(destination=turtle_file_path, format='turtle')\n",
    "\n",
    "print(f\"RDF data exported to {turtle_file_path}\")\n",
    "print(\"Serialization saved successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Person Counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CSV files in memory\n",
    "employee_counts = pd.read_csv(employee_countsUrl, sep=',', index_col='company_record_id')\n",
    "\n",
    "# print(Company)\n",
    "\n",
    "#create the graph\n",
    "g = Graph()\n",
    "\n",
    "# Bind the namespaces to a prefix for more readable output\n",
    "g.bind(\"foaf\", FOAF)\n",
    "g.bind(\"xsd\", XSD)\n",
    "g.bind(\"countries\", CNS)\n",
    "g.bind(\"lnjp\", LNJP)\n",
    "g.bind(\"skos\", SKOS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- saving serialization ---\n",
      "RDF data exported to employee_counts.ttl\n",
      "Serialization saved successfully.\n"
     ]
    }
   ],
   "source": [
    "highFollowers = LNJP.highFollowers\n",
    "follower_count_property = LNJP['follower_count']\n",
    "\n",
    "BNH0 = BNode()\n",
    "BNH1 = BNode()\n",
    "BNH2 = BNode()\n",
    "BNH3 = BNode()\n",
    "BNH4 = BNode()\n",
    "BNH5 = BNode()\n",
    "BNH6 = BNode()\n",
    "BNH7 = BNode()\n",
    "BNH8 = BNode()\n",
    "\n",
    "g.add((highFollowers, RDF.type, OWL.Class))\n",
    "g.add((highFollowers, RDFS.subClassOf, LNJP.Record))\n",
    "\n",
    "g.add((highFollowers, OWL.equivalentClass, BNH0))\n",
    "\n",
    "g.add((BNH0, OWL.intersectionOf, BNH1))\n",
    "\n",
    "g.add((BNH1, RDF.type, RDF.List))\n",
    "g.add((BNH1, RDF.first, LNJP.Record))\n",
    "g.add((BNH1, RDF.rest, BNH2))\n",
    "\n",
    "g.add((BNH2, RDF.type, RDF.List))\n",
    "g.add((BNH2, RDF.first, BNH3))\n",
    "g.add((BNH2, RDF.rest, RDF.nil))\n",
    "\n",
    "g.add((BNH3, RDF.type, OWL.Restriction))\n",
    "g.add((BNH3, OWL.onProperty, follower_count_property))\n",
    "g.add((BNH3, OWL.someValuesFrom, BNH4))\n",
    "\n",
    "g.add((BNH4, RDF.type, RDFS.Datatype))\n",
    "g.add((BNH4, OWL.onDatatype, XSD.integer))\n",
    "g.add((BNH4, OWL.withRestrictions, BNH5))\n",
    "\n",
    "g.add((BNH5, RDF.type, RDF.List))\n",
    "g.add((BNH6, RDF.type, RDF.List))\n",
    "g.add((BNH7, RDF.type, RDF.List))\n",
    "g.add((BNH8, RDF.type, RDF.List))\n",
    "\n",
    "g.add((BNH5, RDF.first, BNH6))\n",
    "g.add((BNH5, RDF.rest, BNH7))\n",
    "\n",
    "g.add((BNH7, RDF.first, BNH8))\n",
    "g.add((BNH7, RDF.rest, RDF.nil))\n",
    "\n",
    "g.add((BNH6, XSD.minInclusive, Literal(\"201083\", datatype=XSD.integer)))\n",
    "\n",
    "lowFollowers = LNJP.lowFollowers\n",
    "follower_count_property = LNJP['follower_count']\n",
    "\n",
    "BNL0 = BNode()\n",
    "BNL1 = BNode()\n",
    "BNL2 = BNode()\n",
    "BNL3 = BNode()\n",
    "BNL4 = BNode()\n",
    "BNL5 = BNode()\n",
    "BNL6 = BNode()\n",
    "BNL7 = BNode()\n",
    "BNL8 = BNode()\n",
    "\n",
    "g.add((lowFollowers, RDF.type, OWL.Class))\n",
    "g.add((lowFollowers, RDFS.subClassOf, LNJP.Record))\n",
    "\n",
    "g.add((lowFollowers, OWL.equivalentClass, BNL0))\n",
    "\n",
    "g.add((BNL0, OWL.intersectionOf, BNL1))\n",
    "\n",
    "g.add((BNL1, RDF.type, RDF.List))\n",
    "g.add((BNL1, RDF.first, LNJP.Record))\n",
    "g.add((BNL1, RDF.rest, BNL2))\n",
    "\n",
    "g.add((BNL2, RDF.type, RDF.List))\n",
    "g.add((BNL2, RDF.first, BNL3))\n",
    "g.add((BNL2, RDF.rest, RDF.nil))\n",
    "\n",
    "g.add((BNL3, RDF.type, OWL.Restriction))\n",
    "g.add((BNL3, OWL.onProperty, follower_count_property))\n",
    "g.add((BNL3, OWL.someValuesFrom, BNL4))\n",
    "\n",
    "g.add((BNL4, RDF.type, RDFS.Datatype))\n",
    "g.add((BNL4, OWL.onDatatype, XSD.integer))\n",
    "g.add((BNL4, OWL.withRestrictions, BNL5))\n",
    "\n",
    "g.add((BNL5, RDF.type, RDF.List))\n",
    "g.add((BNL6, RDF.type, RDF.List))\n",
    "g.add((BNL7, RDF.type, RDF.List))\n",
    "g.add((BNL8, RDF.type, RDF.List))\n",
    "\n",
    "g.add((BNL5, RDF.first, BNL6))\n",
    "g.add((BNL5, RDF.rest, BNL7))\n",
    "\n",
    "g.add((BNL7, RDF.first, BNL8))\n",
    "g.add((BNL7, RDF.rest, RDF.nil))\n",
    "\n",
    "g.add((BNL6, XSD.maxExclusive, Literal(\"201083\", datatype=XSD.integer)))\n",
    "\n",
    "# Iterate over all rows in the companies DataFrame\n",
    "for index, row in employee_counts.iterrows():\n",
    "    # Create Company node with a URI\n",
    "    company_record_id = \"company_record_\" + str(index)\n",
    "    Record = URIRef(LNJP[company_record_id])\n",
    "    g.add((Record, RDF.type, LNJP.Record))\n",
    "\n",
    "    # Add Company information as data properties\n",
    "    g.add((Record, LNJP['company_record_id'], Literal(index, datatype=XSD.integer)))\n",
    "    g.add((Record, LNJP['employee_count'], Literal(int(row['employee_count']), datatype=XSD.integer)))\n",
    "    g.add((Record, LNJP['follower_count'], Literal(int(row['follower_count']), datatype=XSD.integer)))\n",
    "    g.add((Record, LNJP['time_recorded'], Literal(row['time_recorded'], datatype=XSD.date)))\n",
    "\n",
    "    if (Literal(row['follower_count'], datatype=XSD.integer) >= Literal(\"201083\", datatype=XSD.integer)):\n",
    "        g.add((Record,RDF.type,LNJP.highFollowers))\n",
    "    if (Literal(row['follower_count'], datatype=XSD.integer) < Literal(\"201083\", datatype=XSD.integer)):\n",
    "        g.add((Record,RDF.type,LNJP.lowFollowers))\n",
    "\n",
    "    Company = URIRef(LNJP[\"company_\" + str(int(row['company_id']))])\n",
    "    g.add((Record, LNJP['IsForCompany'], Company))\n",
    "# Serialize the RDF graph to Turtle format\n",
    "turtle_file_path = 'employee_counts.ttl'\n",
    "\n",
    "# Check if the file already exists\n",
    "if os.path.exists(turtle_file_path):\n",
    "    user_input = input(f\"The file '{turtle_file_path}' already exists. Do you want to rewrite it? (y/n): \").lower()\n",
    "\n",
    "    if user_input != 'y':\n",
    "        print(\"Serialization not saved. Exiting.\")\n",
    "        # You might want to add additional logic or exit the script if the user chooses not to overwrite the file.\n",
    "        exit()\n",
    "\n",
    "# If the file doesn't exist or the user chose to overwrite, proceed with saving the serialization\n",
    "print(\"--- saving serialization ---\")\n",
    "g.serialize(destination=turtle_file_path, format='turtle')\n",
    "\n",
    "print(f\"RDF data exported to {turtle_file_path}\")\n",
    "print(\"Serialization saved successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benefits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CSV files in memory\n",
    "benefits = pd.read_csv(benefitsUrl, sep=',', index_col='benefit_id')\n",
    "\n",
    "#create the graph\n",
    "g = Graph()\n",
    "\n",
    "# Bind the namespaces to a prefix for more readable output\n",
    "g.bind(\"foaf\", FOAF)\n",
    "g.bind(\"xsd\", XSD)\n",
    "g.bind(\"countries\", CNS)\n",
    "g.bind(\"lnjp\", LNJP)\n",
    "g.bind(\"skos\", SKOS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- saving serialization ---\n",
      "RDF data exported to benefits.ttl\n",
      "Serialization saved successfully.\n"
     ]
    }
   ],
   "source": [
    "# Iterate over all rows in the companies DataFrame\n",
    "for index, row in benefits.iterrows():\n",
    "    # Create Company node with a URI\n",
    "    benefit_id = \"benefit_\" + str(index)\n",
    "    Benefit = URIRef(LNJP[benefit_id])\n",
    "    g.add((Benefit, RDF.type, LNJP.Benefit))\n",
    "\n",
    "    # Add Company information as data properties\n",
    "    g.add((Benefit, LNJP['benefit_id'], Literal(index, datatype=XSD.integer)))\n",
    "    g.add((Benefit, LNJP['inferred'], Literal(row['inferred'], datatype=XSD.boolean)))\n",
    "    g.add((Benefit, LNJP['type'], Literal(row['type'], datatype=XSD.string)))\n",
    "\n",
    "    JobPosting = URIRef(LNJP[\"job_\" + str(row['job_id'])])\n",
    "    g.add((Benefit, LNJP['isForJobPosting'], JobPosting))\n",
    "    \n",
    "# Serialize the RDF graph to Turtle format\n",
    "turtle_file_path = 'benefits.ttl'\n",
    "\n",
    "# Check if the file already exists\n",
    "if os.path.exists(turtle_file_path):\n",
    "    user_input = input(f\"The file '{turtle_file_path}' already exists. Do you want to rewrite it? (y/n): \").lower()\n",
    "\n",
    "    if user_input != 'y':\n",
    "        print(\"Serialization not saved. Exiting.\")\n",
    "        # You might want to add additional logic or exit the script if the user chooses not to overwrite the file.\n",
    "        exit()\n",
    "\n",
    "# If the file doesn't exist or the user chose to overwrite, proceed with saving the serialization\n",
    "print(\"--- saving serialization ---\")\n",
    "g.serialize(destination=turtle_file_path, format='turtle')\n",
    "\n",
    "print(f\"RDF data exported to {turtle_file_path}\")\n",
    "print(\"Serialization saved successfully.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specialities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CSV files in memory\n",
    "specialities = pd.read_csv(specialitiesUrl, sep=',', index_col='speciality_id', keep_default_na=False, na_values=['_'])\n",
    "\n",
    "#create the graph\n",
    "g = Graph()\n",
    "\n",
    "# Bind the namespaces to a prefix for more readable output\n",
    "g.bind(\"foaf\", FOAF)\n",
    "g.bind(\"xsd\", XSD)\n",
    "g.bind(\"countries\", CNS)\n",
    "g.bind(\"lnjp\", LNJP)\n",
    "g.bind(\"skos\", SKOS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- saving serialization ---\n",
      "RDF data exported to speciality.ttl\n",
      "Serialization saved successfully.\n"
     ]
    }
   ],
   "source": [
    "# Iterate over all rows in the companies DataFrame\n",
    "for index, row in specialities.iterrows():\n",
    "    # Create Company node with a URI\n",
    "    speciality_id = \"speciality_\" + str(index)\n",
    "    Speciality = URIRef(LNJP[speciality_id])\n",
    "    g.add((Speciality, RDF.type, LNJP.Speciality))\n",
    "\n",
    "    # Add Company information as data properties\n",
    "    g.add((Speciality, LNJP['speciality_id'], Literal(index, datatype=XSD.integer)))\n",
    "    g.add((Speciality, LNJP['speciality'], Literal(row['speciality'], datatype=XSD.string)))\n",
    "# Serialize the RDF graph to Turtle format\n",
    "turtle_file_path = 'speciality.ttl'\n",
    "\n",
    "# Check if the file already exists\n",
    "if os.path.exists(turtle_file_path):\n",
    "    user_input = input(f\"The file '{turtle_file_path}' already exists. Do you want to rewrite it? (y/n): \").lower()\n",
    "\n",
    "    if user_input != 'y':\n",
    "        print(\"Serialization not saved. Exiting.\")\n",
    "        # You might want to add additional logic or exit the script if the user chooses not to overwrite the file.\n",
    "        exit()\n",
    "\n",
    "# If the file doesn't exist or the user chose to overwrite, proceed with saving the serialization\n",
    "print(\"--- saving serialization ---\")\n",
    "g.serialize(destination=turtle_file_path, format='turtle')\n",
    "\n",
    "print(f\"RDF data exported to {turtle_file_path}\")\n",
    "print(\"Serialization saved successfully.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Company_Specialities Join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CSV files in memory\n",
    "company_specialities = pd.read_csv(company_specialitiesUrl, sep=',', index_col='company_id', keep_default_na=False, na_values=['_'])\n",
    "\n",
    "#create the graph\n",
    "g = Graph()\n",
    "\n",
    "# Bind the namespaces to a prefix for more readable output\n",
    "g.bind(\"foaf\", FOAF)\n",
    "g.bind(\"xsd\", XSD)\n",
    "g.bind(\"countries\", CNS)\n",
    "g.bind(\"lnjp\", LNJP)\n",
    "g.bind(\"skos\", SKOS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- saving serialization ---\n",
      "RDF data exported to company_specialities_join.ttl\n",
      "Serialization saved successfully.\n"
     ]
    }
   ],
   "source": [
    "# Assuming you have a DataFrame called 'skillsDataFrame' with columns 'skill_abr' and 'skill_name'\n",
    "# Iterate over all rows in the skills DataFrame\n",
    "for index, row in company_specialities.iterrows():\n",
    "    # Create Skill node with a URI\n",
    "    # skillId = \"skill_\" + str(skill_abr)\n",
    "    \n",
    "    company_id = \"company_\" + str(index)\n",
    "    Company = URIRef(LNJP[company_id])\n",
    "    Speciality = URIRef(LNJP[\"speciality_\" + str(row['speciality_id'])])\n",
    "    g.add((Company, LNJP['hasSpeciality'], Speciality))\n",
    "    \n",
    "# Serialize the RDF graph to Turtle format\n",
    "turtle_file_path = 'company_specialities_join.ttl'\n",
    "\n",
    "# Check if the file already exists\n",
    "if os.path.exists(turtle_file_path):\n",
    "    user_input = input(f\"The file '{turtle_file_path}' already exists. Do you want to rewrite it? (y/n): \").lower()\n",
    "\n",
    "    if user_input != 'y':\n",
    "        print(\"Serialization not saved. Exiting.\")\n",
    "        # You might want to add additional logic or exit the script if the user chooses not to overwrite the file.\n",
    "        exit()\n",
    "\n",
    "# If the file doesn't exist or the user chose to overwrite, proceed with saving the serialization\n",
    "print(\"--- saving serialization ---\")\n",
    "g.serialize(destination=turtle_file_path, format='turtle')\n",
    "\n",
    "print(f\"RDF data exported to {turtle_file_path}\")\n",
    "print(\"Serialization saved successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Salaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CSV files in memory\n",
    "salaries = pd.read_csv(salariesUrl, sep=',', index_col='salary_id')\n",
    "\n",
    "#create the graph\n",
    "g = Graph()\n",
    "\n",
    "# Bind the namespaces to a prefix for more readable output\n",
    "g.bind(\"foaf\", FOAF)\n",
    "g.bind(\"xsd\", XSD)\n",
    "g.bind(\"countries\", CNS)\n",
    "g.bind(\"lnjp\", LNJP)\n",
    "g.bind(\"skos\", SKOS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- saving serialization ---\n",
      "RDF data exported to salaries.ttl\n",
      "Serialization saved successfully.\n"
     ]
    }
   ],
   "source": [
    "# Iterate over all rows in the companies DataFrame\n",
    "for index, row in salaries.iterrows():\n",
    "    # Create Company node with a URI\n",
    "    salary_id = \"salary_\" + str(index)\n",
    "    Salary = URIRef(LNJP[salary_id])\n",
    "    g.add((Salary, RDF.type, LNJP.Salary))\n",
    "\n",
    "    # Add Company information as data properties\n",
    "    g.add((Salary, LNJP['salary_id'], Literal(index, datatype=XSD.integer)))\n",
    "    if not pd.isnull(row['max_salary']) and row['max_salary'] != \"\":\n",
    "        g.add((Salary, LNJP['max_salary'], Literal(row['max_salary'], datatype=XSD.decimal)))\n",
    "    if not pd.isnull(row['med_salary']) and row['med_salary'] != \"\":        \n",
    "        g.add((Salary, LNJP['med_salary'], Literal(row['med_salary'], datatype=XSD.decimal)))\n",
    "    if not pd.isnull(row['min_salary']) and row['min_salary'] != \"\":    \n",
    "        g.add((Salary, LNJP['min_salary'], Literal(row['min_salary'], datatype=XSD.decimal)))\n",
    "    g.add((Salary, LNJP['pay_period'], Literal(row['pay_period'], datatype=XSD.string)))\n",
    "    g.add((Salary, LNJP['currency'], Literal(row['currency'], datatype=XSD.string)))\n",
    "    g.add((Salary, LNJP['compensation_type'], Literal(row['compensation_type'], datatype=XSD.string)))\n",
    "\n",
    "    JobPosting = URIRef(LNJP[\"job_\" + str(row['job_id'])])\n",
    "    g.add((Salary, LNJP['isAllocatedFor'], JobPosting))\n",
    "# Serialize the RDF graph to Turtle format\n",
    "turtle_file_path = 'salaries.ttl'\n",
    "\n",
    "# Check if the file already exists\n",
    "if os.path.exists(turtle_file_path):\n",
    "    user_input = input(f\"The file '{turtle_file_path}' already exists. Do you want to rewrite it? (y/n): \").lower()\n",
    "\n",
    "    if user_input != 'y':\n",
    "        print(\"Serialization not saved. Exiting.\")\n",
    "        # You might want to add additional logic or exit the script if the user chooses not to overwrite the file.\n",
    "        exit()\n",
    "\n",
    "# If the file doesn't exist or the user chose to overwrite, proceed with saving the serialization\n",
    "print(\"--- saving serialization ---\")\n",
    "g.serialize(destination=turtle_file_path, format='turtle')\n",
    "\n",
    "print(f\"RDF data exported to {turtle_file_path}\")\n",
    "print(\"Serialization saved successfully.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
