{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Populate the Job Posting Ontology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# required libraries\n",
    "import pandas as pd\n",
    "import os\n",
    "from pathlib import Path\n",
    "# Load the required libraries\n",
    "from rdflib import Graph, Literal, RDF, URIRef, Namespace\n",
    "# rdflib knows about some namespaces, like FOAF\n",
    "from rdflib.namespace import FOAF, XSD\n",
    "# CHECK DATE \n",
    "import datetime\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/alitahvildari/Documents\n",
      "data/job_posting.csv\n"
     ]
    }
   ],
   "source": [
    "# parameters and URLs\n",
    "path = str(Path(os.path.abspath(os.getcwd())).parent.absolute())\n",
    "print(path)\n",
    "jobPostingUrl = 'data/job_posting.csv'\n",
    "print(jobPostingUrl)\n",
    "\n",
    "companiesUrl = path + '/GraphDB/data/company_details/companies.csv'\n",
    "company_industriesUrl = path + '/GraphDB/data/company_details/company_industries.csv'\n",
    "company_specialitiesUrl = path + '/GraphDB/data/company_details/company_specialities.csv'\n",
    "employee_countsUrl = path + '/GraphDB/data/company_details/employee_counts.csv'\n",
    "\n",
    "benefitsUrl = path + '/GraphDB/data/job_details/benefits.csv'\n",
    "job_industriesUrl = path + '/GraphDB/data/job_details/job_industries.csv'\n",
    "job_skillsUrl = path + '/GraphDB/data/job_details/job_skills.csv'\n",
    "salariesUrl = path + '/GraphDB/data/job_details/salaries.csv'\n",
    "\n",
    "industriesUrl = path + '/GraphDB/data/maps/industries.csv'\n",
    "skillsUrl = path + '/GraphDB/data/maps/skills.csv'\n",
    "\n",
    "\n",
    "# country codes\n",
    "countriesURL = path + '/GraphDB/data/countries/all.csv'\n",
    "\n",
    "# saving folder\n",
    "savePath =  path + '/data/rdf/linkedinDB/' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct the country and the movie ontology namespaces not known by RDFlib\n",
    "CNS = Namespace(\"http://eulersharp.sourceforge.net/2003/03swap/countries#\")\n",
    "LNJP = Namespace(\"http://www.dei.unipd.it/database2/LinkedinJobPosting#\")\n",
    "SKOS = Namespace(\"https://www.w3.org/2009/08/skos-reference/skos-owl1-dl.rdf\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Job Posting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CSV files in memory\n",
    "jobPosting = pd.read_csv(jobPostingUrl, sep=',', index_col='job_id')\n",
    "#print(jobPosting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create the graph\n",
    "g = Graph()\n",
    "\n",
    "# Bind the namespaces to a prefix for more readable output\n",
    "g.bind(\"foaf\", FOAF)\n",
    "g.bind(\"xsd\", XSD)\n",
    "g.bind(\"countries\", CNS)\n",
    "g.bind(\"lnjp\", LNJP)\n",
    "g.bind(\"skos\", SKOS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- saving serialization ---\n",
      "RDF data exported to job_postings.ttl\n",
      "Serialization saved successfully.\n"
     ]
    }
   ],
   "source": [
    "# Iterate over the job postings DataFrame\n",
    "for index, row in jobPosting.iterrows():\n",
    "    # Create the JobPosting node with a URI\n",
    "    jobPostingId = \"job_posting\" + str(index)\n",
    "    JobPosting = URIRef(LNJP[jobPostingId])\n",
    "    g.add((JobPosting, RDF.type, LNJP.JobPosting))\n",
    "\n",
    "\n",
    "    #---- for check ---\n",
    "\n",
    "    # TransfermarktProfile = URIRef(row['url'])\n",
    "    \n",
    "    # # Add triples using store's add() method.\n",
    "    # g.add((Player, RDF.type, SO.Player))\n",
    "    # g.add((TransfermarktProfile, RDF.type, SO.TransfermarktProfile))\n",
    "    # g.add((TransfermarktProfile, SO['isAbout'], Player))\n",
    "\n",
    "    #------------------\n",
    "\n",
    "    # Add data properties\n",
    "    g.add((JobPosting, LNJP.sponsored, Literal(row['sponsored'], datatype=XSD.boolean)))\n",
    "    g.add((JobPosting, LNJP.original_listed_time, Literal(row['original_listed_time'], datatype=XSD.dateTime)))\n",
    "    g.add((JobPosting, LNJP.applies, Literal(row['applies'], datatype=XSD.integer)))\n",
    "    g.add((JobPosting, LNJP.expiry, Literal(row['expiry'], datatype=XSD.dateTime)))\n",
    "    g.add((JobPosting, LNJP.min_salary, Literal(row['min_salary'], datatype=XSD.integer)))\n",
    "    g.add((JobPosting, LNJP.max_salary, Literal(row['max_salary'], datatype=XSD.integer)))\n",
    "    g.add((JobPosting, LNJP.location, Literal(row['location'], datatype=XSD.string)))\n",
    "    g.add((JobPosting, LNJP.work_type, Literal(row['work_type'], datatype=XSD.string)))\n",
    "    g.add((JobPosting, LNJP.posting_domain, Literal(row['posting_domain'], datatype=XSD.string)))\n",
    "    g.add((JobPosting, LNJP.compensation_type, Literal(row['compensation_type'], datatype=XSD.string)))\n",
    "    g.add((JobPosting, LNJP.med_salary, Literal(row['med_salary'], datatype=XSD.integer)))\n",
    "    g.add((JobPosting, LNJP.skills_desc, Literal(row['skills_desc'], datatype=XSD.string)))\n",
    "    g.add((JobPosting, LNJP.remote_allowed, Literal(row['remote_allowed'], datatype=XSD.boolean)))\n",
    "    g.add((JobPosting, LNJP.application_url, Literal(row['application_url'], datatype=XSD.string)))\n",
    "    g.add((JobPosting, LNJP.pay_period, Literal(row['pay_period'], datatype=XSD.string)))\n",
    "    g.add((JobPosting, LNJP.listed_time, Literal(row['listed_time'], datatype=XSD.dateTime)))\n",
    "    g.add((JobPosting, LNJP.currency, Literal(row['currency'], datatype=XSD.string)))\n",
    "    g.add((JobPosting, LNJP.views, Literal(row['views'], datatype=XSD.integer)))\n",
    "    g.add((JobPosting, LNJP.company_id, Literal(row['company_id'], datatype=XSD.int)))\n",
    "    #g.add((JobPosting, LNJP.job_id, Literal(row['job_id'], datatype=XSD.int)))\n",
    "    g.add((JobPosting, LNJP.job_id, Literal(index, datatype=XSD.int)))\n",
    "    g.add((JobPosting, LNJP.application_type, Literal(row['application_type'], datatype=XSD.string)))\n",
    "    g.add((JobPosting, LNJP.job_posting_url, Literal(row['job_posting_url'], datatype=XSD.string)))\n",
    "    g.add((JobPosting, LNJP.title, Literal(row['title'], datatype=XSD.string)))\n",
    "    g.add((JobPosting, LNJP.formatted_experience_level, Literal(row['formatted_experience_level'], datatype=XSD.string)))\n",
    "    g.add((JobPosting, LNJP.closed_time, Literal(row['closed_time'], datatype=XSD.dateTime)))\n",
    "    g.add((JobPosting, LNJP.description, Literal(row['description'], datatype=XSD.string)))\n",
    "\n",
    "\n",
    "# terminated\n",
    "# jobSkillsAbr = \"skill_\" + str(row['abr']) # You should replace this with the actual ID or URI\n",
    "# JobSkills = URIRef(LNJP[jobSkillsAbr])\n",
    "# print(JobSkills)\n",
    "\n",
    "\n",
    "# Serialize the RDF graph to Turtle format\n",
    "turtle_file_path = 'job_postings.ttl'\n",
    "\n",
    "# Check if the file already exists\n",
    "if os.path.exists(turtle_file_path):\n",
    "    user_input = input(f\"The file '{turtle_file_path}' already exists. Do you want to rewrite it? (y/n): \").lower()\n",
    "    \n",
    "    if user_input != 'y':\n",
    "        print(\"Serialization not saved. Exiting.\")\n",
    "        # You might want to add additional logic or exit the script if the user chooses not to overwrite the file.\n",
    "        exit()\n",
    "\n",
    "# If the file doesn't exist or the user chose to overwrite, proceed with saving the serialization\n",
    "print(\"--- saving serialization ---\")\n",
    "g.serialize(destination=turtle_file_path, format='turtle')\n",
    "\n",
    "print(f\"RDF data exported to {turtle_file_path}\")\n",
    "print(\"Serialization saved successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- saving serialization ---\n",
      "Serialization saved successfully.\n"
     ]
    }
   ],
   "source": [
    "# # Check if the file already exists\n",
    "# if os.path.exists(turtle_file_path):\n",
    "#     user_input = input(f\"The file '{turtle_file_path}' already exists. Do you want to rewrite it? (y/n): \").lower()\n",
    "    \n",
    "#     if user_input != 'y':\n",
    "#         print(\"Serialization not saved. Exiting.\")\n",
    "#         # You might want to add additional logic or exit the script if the user chooses not to overwrite the file.\n",
    "#         exit()\n",
    "\n",
    "# # If the file doesn't exist or the user chose to overwrite, proceed with saving the serialization\n",
    "# print(\"--- saving serialization ---\")\n",
    "# with open(turtle_file_path, 'w') as file:\n",
    "#     file.write(g.serialize(format='turtle'))\n",
    "\n",
    "# print(\"Serialization saved successfully.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Skills"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skills DataFrame read successfully.\n"
     ]
    }
   ],
   "source": [
    "# Load the CSV files in memory\n",
    "skills = pd.read_csv(skillsUrl, sep=',', index_col='skill_abr')\n",
    "assert not skills.empty, \"Skills DataFrame is empty. Check the CSV file or URL.\"\n",
    "\n",
    "# Print a message if the test passes\n",
    "print(\"Skills DataFrame read successfully.\")\n",
    "#create the graph\n",
    "g = Graph()\n",
    "\n",
    "# Bind the namespaces to a prefix for more readable output\n",
    "g.bind(\"foaf\", FOAF)\n",
    "g.bind(\"xsd\", XSD)\n",
    "g.bind(\"countries\", CNS)\n",
    "g.bind(\"lnjp\", LNJP)\n",
    "g.bind(\"skos\", SKOS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'JobPosting' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/alitahvildari/Documents/GraphDB/loadJobPosting -c.ipynb Cell 12\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/alitahvildari/Documents/GraphDB/loadJobPosting%20-c.ipynb#X14sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     g\u001b[39m.\u001b[39madd((Skill, LNJP\u001b[39m.\u001b[39mskill_name, Literal(row[\u001b[39m'\u001b[39m\u001b[39mskill_name\u001b[39m\u001b[39m'\u001b[39m], datatype\u001b[39m=\u001b[39mXSD\u001b[39m.\u001b[39mstring)))\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/alitahvildari/Documents/GraphDB/loadJobPosting%20-c.ipynb#X14sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     \u001b[39m# Connect the JobPosting node to the Skill using the hasSkill relation\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/alitahvildari/Documents/GraphDB/loadJobPosting%20-c.ipynb#X14sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     g\u001b[39m.\u001b[39madd((JobPosting, LNJP\u001b[39m.\u001b[39mhasSkill, Skill))\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/alitahvildari/Documents/GraphDB/loadJobPosting%20-c.ipynb#X14sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m \u001b[39m# Serialize the RDF graph to Turtle format\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/alitahvildari/Documents/GraphDB/loadJobPosting%20-c.ipynb#X14sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m turtle_file_path \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mskills.ttl\u001b[39m\u001b[39m'\u001b[39m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'JobPosting' is not defined"
     ]
    }
   ],
   "source": [
    "# Assuming you have a DataFrame called 'skillsDataFrame' with columns 'skill_abr' and 'skill_name'\n",
    "# Iterate over all rows in the skills DataFrame\n",
    "for skill_abr, row in skills.iterrows():\n",
    "    # Create Skill node with a URI\n",
    "    skillId = \"skill_\" + str(skill_abr)\n",
    "    Skill = URIRef(LNJP[skillId])\n",
    "    g.add((Skill, RDF.type, LNJP.Skill))\n",
    "\n",
    "    # Add skill information as data properties\n",
    "    g.add((Skill, LNJP.skill_abr, Literal(skill_abr, datatype=XSD.string)))\n",
    "    g.add((Skill, LNJP.skill_name, Literal(row['skill_name'], datatype=XSD.string)))\n",
    "    \n",
    "\n",
    "# Serialize the RDF graph to Turtle format\n",
    "turtle_file_path = 'skills.ttl'\n",
    "\n",
    "# Check if the file already exists\n",
    "if os.path.exists(turtle_file_path):\n",
    "    user_input = input(f\"The file '{turtle_file_path}' already exists. Do you want to rewrite it? (y/n): \").lower()\n",
    "\n",
    "    if user_input != 'y':\n",
    "        print(\"Serialization not saved. Exiting.\")\n",
    "        # You might want to add additional logic or exit the script if the user chooses not to overwrite the file.\n",
    "        exit()\n",
    "\n",
    "# If the file doesn't exist or the user chose to overwrite, proceed with saving the serialization\n",
    "print(\"--- saving serialization ---\")\n",
    "g.serialize(destination=turtle_file_path, format='turtle')\n",
    "\n",
    "print(f\"RDF data exported to {turtle_file_path}\")\n",
    "print(\"Serialization saved successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- saving serialization ---\n",
      "CPU times: user 22.1 ms, sys: 2.59 ms, total: 24.7 ms\n",
      "Wall time: 23.7 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# print all the data in the Turtle format\n",
    "print(\"--- saving serialization ---\")\n",
    "with open('soccerDB/clubs.ttl', 'w') as file:\n",
    "    file.write(g.serialize(format='turtle'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Players"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CSV files in memory\n",
    "# Load players data\n",
    "players = pd.read_csv(playersUrl, sep=',', index_col='player_id', keep_default_na=False, na_values=['_'])\n",
    "# Load playersFifa data\n",
    "playersFifa = pd.read_csv(playersFifaUrl, sep=',', index_col='sofifa_id', keep_default_na=False, na_values=['_'])\n",
    "\n",
    "# Create the graph\n",
    "g = Graph()\n",
    "\n",
    "# Bind the namespaces to a prefix for more readable output\n",
    "g.bind(\"foaf\", FOAF)\n",
    "g.bind(\"xsd\", XSD)\n",
    "g.bind(\"countries\", CNS)\n",
    "g.bind(\"so\", SO)\n",
    "\n",
    "# Turtle comments for the code above\n",
    "# Players data:\n",
    "# :player1 rdf:type so:Player .\n",
    "# :player1 so:hasAttribute value1 .\n",
    "# :player1 so:hasAttribute value2 .\n",
    "# ...\n",
    "\n",
    "# PlayersFifa data:\n",
    "# :fifaPlayer1 rdf:type so:FifaPlayer .\n",
    "# :fifaPlayer1 so:hasFifaAttribute value1 .\n",
    "# :fifaPlayer1 so:hasFifaAttribute value2 .\n",
    "# ...\n",
    "\n",
    "# Note: Replace :player1, :fifaPlayer1, etc., with actual URIs or blank node identifiers.\n",
    "# Replace so:hasAttribute and so:hasFifaAttribute with actual properties.\n",
    "# Replace value1, value2, etc., with actual values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the country codes\n",
    "# we need to convert NaN values to something else otherwise NA strings are converted to NaN -> problem with Namibia\n",
    "countries = pd.read_csv(countriesURL, sep=',', index_col='name', keep_default_na=False, na_values=['_'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from difflib import SequenceMatcher\n",
    "import numpy as np\n",
    "\n",
    "import unicodedata\n",
    "def strip_accents(s):\n",
    "   return ''.join(c for c in unicodedata.normalize('NFD', s)\n",
    "                  if unicodedata.category(c) != 'Mn')\n",
    "\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 15.5 s, sys: 86.3 ms, total: 15.5 s\n",
      "Wall time: 15.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "#measure execution time\n",
    "\n",
    "#iterate over the players dataframe\n",
    "for index, row in players.iterrows():\n",
    "    # Create the node to add to the Graph\n",
    "    # the node has the namespace + the player id as URI\n",
    "    idU = \"player\"+str(index)\n",
    "    Player = URIRef(SO[idU])\n",
    "    # the transferMarkt profile has as URI, the URL of the profile in the website\n",
    "    TransfermarktProfile = URIRef(row['url'])\n",
    "    \n",
    "    # Add triples using store's add() method.\n",
    "    g.add((Player, RDF.type, SO.Player))\n",
    "    g.add((TransfermarktProfile, RDF.type, SO.TransfermarktProfile))\n",
    "    g.add((TransfermarktProfile, SO['isAbout'], Player))\n",
    "    \n",
    "    #process player name\n",
    "    name = row['name'].split('-')\n",
    "\n",
    "    if (len(name)>1):\n",
    "        g.add((Player, SO['firstName'], Literal(name[0], datatype=XSD.string)))\n",
    "        g.add((Player, SO['lastName'], Literal(name[1], datatype=XSD.string)))\n",
    "    else:\n",
    "        g.add((Player, SO['lastName'], Literal(name[0], datatype=XSD.string)))\n",
    "        \n",
    "    #there can be more than one position per player\n",
    "    for pos in row['position'].split(' - '):\n",
    "        g.add((Player, SO['position'], Literal(pos.lower(), datatype=XSD.string)))\n",
    "    \n",
    "    if not(row['club_id']==''):\n",
    "        idC = \"club\"+str(row['club_id'])\n",
    "        g.add((Player, SO['playFor'], URIRef(SO[idC])))\n",
    "\n",
    "#iterate over the fifa dataframe\n",
    "for index, row in playersFifa.iterrows():\n",
    "    pname = row['short_name'].lower()\n",
    "    if ('.' in pname):\n",
    "        # get last name\n",
    "        # in the fifa dataset we have short names as L. Messi so we delete the L. \n",
    "        # we need to check if the last name contains a space\n",
    "        pname = row['short_name'].split('.')[1].lower().strip()\n",
    "        if ' ' in pname:\n",
    "            i = 0\n",
    "            for t in pname.split(' '):\n",
    "                if i == 0:\n",
    "                    pname = t.lower()\n",
    "                else:\n",
    "                    pname = pname + \"-\" + t.lower()\n",
    "                i += 1           \n",
    "    elif(' ' in pname):\n",
    "        # here we have to handle Cristiano Ronaldo mapping it to cristiano-ronaldo to maximize the match in the players dataframe \n",
    "        i = 0\n",
    "        for t in row['short_name'].split(' '):\n",
    "            if i == 0:\n",
    "                pname = t.lower()\n",
    "            else:\n",
    "                pname = pname + \"-\" + t.lower()\n",
    "            i += 1\n",
    "    pname = strip_accents(pname)\n",
    "    \n",
    "    # find sim with the full name \n",
    "    fullname = row['long_name'].lower()\n",
    "    i = 0\n",
    "    for t in fullname.split(' '):\n",
    "        if i == 0:\n",
    "            fullname = t.lower()\n",
    "        else:\n",
    "            fullname = fullname + \"-\" + t.lower()\n",
    "        i += 1 \n",
    "    fullname = strip_accents(fullname)\n",
    "    # check the players with that last name\n",
    "    names =  players[players['name'].str.contains(pname)]['name']\n",
    "    #find max similarity    \n",
    "    maxN = 0\n",
    "    playerId = ''\n",
    "    for n in names:\n",
    "        sim = SequenceMatcher(None, fullname, n).ratio()\n",
    "        if (maxN < sim):\n",
    "            maxN = sim\n",
    "            playerId = players.loc[players['name'] == n].index[0]\n",
    "        \n",
    "    #if we get a valid playerId we can connect the Fifa stats to the transfermrkt player\n",
    "    if (playerId != ''):\n",
    "        #remove the row from the player dataframe to avoid futher matchings (we know data will contain errors)\n",
    "        players = players.drop(index=playerId)\n",
    "        idU = \"player\"+str(playerId)\n",
    "        Player = URIRef(SO[idU])\n",
    "        g.add((Player, SO['overallFifaValue'], Literal(row['overall'], datatype=XSD.int)))\n",
    "        g.add((Player, SO['growthFifaPotential'], Literal(row['potential'], datatype=XSD.int)))\n",
    "        g.add((Player, SO['economicValue'], Literal(row['value_eur'], datatype=XSD.int)))\n",
    "        g.add((Player, SO['annualWage'], Literal(row['wage_eur'], datatype=XSD.int))) \n",
    "        \n",
    "        pFeatures = str(row['player_tags'])\n",
    "        if pFeatures != '_' and pFeatures != '':\n",
    "            pFeatures = pFeatures.split(',')\n",
    "            for feature in pFeatures:\n",
    "                feature = feature.strip()\n",
    "                feature = re.sub('#', '', feature)\n",
    "                g.add((Player, SO['playerFeature'], Literal(feature, datatype=XSD.string)))\n",
    "        \n",
    "        if row['contract_valid_until'] != '_' and row['contract_valid_until'] != '':\n",
    "            g.add((Player, SO['contractValidTo'], Literal(int(row['contract_valid_until']), datatype=XSD.gYear)))        \n",
    "\n",
    "        g.add((Player, SO['birthday'], Literal(row['dob'], datatype=XSD.date)))\n",
    "        g.add((Player, SO['height'], Literal(row['height_cm'], datatype=XSD.int)))\n",
    "        g.add((Player, SO['weight'], Literal(row['weight_kg'], datatype=XSD.int)))\n",
    "        \n",
    "        \n",
    "        nationality = row['nationality'] \n",
    "        nationality = nationality.replace(\" \", \"_\")\n",
    "        # create the RDF node\n",
    "        Country = URIRef(CNS[nationality])\n",
    "        # add the edge connecting the Movie and the Country \n",
    "        g.add((Player, SO['nationality'], Country))   \n",
    "\n",
    "        # Homework: extend the code to populate the 'propertyOf' edge\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- saving serialization ---\n",
      "CPU times: user 1.51 s, sys: 8.25 ms, total: 1.52 s\n",
      "Wall time: 1.52 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# print all the data in the Turtle format\n",
    "print(\"--- saving serialization ---\")\n",
    "with open(savePath + 'players.ttl', 'w') as file:\n",
    "    file.write(g.serialize(format='turtle'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CSV files in memory\n",
    "apps = pd.read_csv(appearancesUrl, sep=',', index_col='appearance_id', keep_default_na=False, na_values=['_'])\n",
    "games = pd.read_csv(gamesUrl, sep=',', index_col='game_id', keep_default_na=False, na_values=['_'])\n",
    "\n",
    "#create the graph\n",
    "g = Graph()\n",
    "\n",
    "# Bind the namespaces to a prefix for more readable output\n",
    "g.bind(\"foaf\", FOAF)\n",
    "g.bind(\"xsd\", XSD)\n",
    "g.bind(\"countries\", CNS)\n",
    "g.bind(\"so\", SO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#iterate over the games dataframe\n",
    "for index, row in games.iterrows():\n",
    "    # we use the transfermrket URL as URI\n",
    "    Game = URIRef(row['url'])\n",
    "    g.add((Game, RDF.type, SO.Game))\n",
    "    idU1 = \"club\"+str(row['home_club_id'])\n",
    "    idU2 = \"club\"+str(row['away_club_id'])\n",
    "    HomeClub = URIRef(SO[idU1])\n",
    "    AwayClub = URIRef(SO[idU2])\n",
    "    g.add((Game, SO['homeClub'], HomeClub))\n",
    "    g.add((Game, SO['awayClub'], AwayClub))    \n",
    "    g.add((Game, SO['matchDay'], Literal(row['date'], datatype=XSD.date)))\n",
    "    g.add((Game, SO['homeClubGoals'], Literal(row['home_club_goals'], datatype=XSD.int)))\n",
    "    g.add((Game, SO['awayClubGoals'], Literal(row['away_club_goals'], datatype=XSD.int)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- saving serialization ---\n",
      "CPU times: user 404 ms, sys: 3.27 ms, total: 407 ms\n",
      "Wall time: 409 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# print all the data in the Turtle format\n",
    "print(\"--- saving serialization ---\")\n",
    "with open(savePath + 'games.ttl', 'w') as file:\n",
    "    file.write(g.serialize(format='turtle'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reload the full players dataframe\n",
    "players = pd.read_csv(playersUrl, sep=',', index_col='player_id', keep_default_na=False, na_values=['_'])\n",
    "#create the graph\n",
    "g = Graph()\n",
    "\n",
    "# Bind the namespaces to a prefix for more readable output\n",
    "g.bind(\"foaf\", FOAF)\n",
    "g.bind(\"xsd\", XSD)\n",
    "g.bind(\"countries\", CNS)\n",
    "g.bind(\"so\", SO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oldgameid = ''\n",
    "for index, row in apps.iterrows():\n",
    "    idA = \"appearance\"+str(index)\n",
    "    idP = \"player\"+str(row['player_id'])\n",
    "    Appearance = URIRef(SO[idA])\n",
    "    Player = URIRef(SO[idP])\n",
    "    currgameid = str(row['game_id'])\n",
    "    idG = \"game\"+currgameid\n",
    "    Game = URIRef(SO[idG])\n",
    "    g.add((Appearance, RDF.type, SO.Appearance))\n",
    "    g.add((Player, SO['appearIn'], Appearance))\n",
    "    g.add((Appearance, SO['playIn'], Game))\n",
    "\n",
    "    g.add((Appearance, SO['goals'], Literal(row['goals'], datatype=XSD.int)))\n",
    "    g.add((Appearance, SO['assists'], Literal(row['assists'], datatype=XSD.int)))\n",
    "    g.add((Appearance, SO['minutesPlayed'], Literal(row['minutes_played'], datatype=XSD.int)))\n",
    "    g.add((Appearance, SO['yellowCard'], Literal(row['yellow_cards'], datatype=XSD.int)))\n",
    "    g.add((Appearance, SO['redCard'], Literal(row['red_cards'], datatype=XSD.int)))\n",
    "\n",
    "    #add this triple only once per game\n",
    "    if (currgameid != oldgameid):\n",
    "        idL = \"league\"+str(row['league_id'])\n",
    "        g.add((Game, SO['belongTo'], URIRef(SO[idL])))\n",
    "        oldgameid = currgameid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- saving serialization ---\n",
      "CPU times: user 12.2 s, sys: 48.5 ms, total: 12.3 s\n",
      "Wall time: 12.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# print all the data in the Turtle format\n",
    "print(\"--- saving serialization ---\")\n",
    "with open(savePath + 'appearances.ttl', 'w') as file:\n",
    "    file.write(g.serialize(format='turtle'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
