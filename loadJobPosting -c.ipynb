{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning\n",
    "### No need to run the data cleaning section. This section is just for indicating that we have used pre-processing on our data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The provided Python script identifies unique industry names from 'companies_industry.csv' absent in 'industries.csv', assigns new IDs, and adds these unique industries to 'industries.csv' and change the column \"industry\" to \"industry_id\" in 'company_industries.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "industries_df = pd.read_csv('industries.csv')\n",
    "companies_industry_df = pd.read_csv('company_industries.csv')\n",
    "\n",
    "\n",
    "unknown_industries = companies_industry_df[~companies_industry_df['industry'].isin(industries_df['industry_name'])]['industry']\n",
    "\n",
    "# Count unique industry names that are not present in industries.csv\n",
    "unique_unknown_industries = unknown_industries.nunique()\n",
    "print(f\"Number of unique industry names to be added to industries.csv: {unique_unknown_industries}\")\n",
    "\n",
    "# Create DataFrame for unique unknown industries with new IDs\n",
    "new_ids = range(industries_df['industry_id'].max() + 1, industries_df['industry_id'].max() + 1 + unique_unknown_industries)\n",
    "new_industries = pd.DataFrame({'industry_id': new_ids, 'industry_name': unknown_industries.unique()})\n",
    "\n",
    "\n",
    "industries_df = pd.concat([industries_df, new_industries], ignore_index=True)\n",
    "\n",
    "# Merge industries_df and companies_industry_df on 'industry' to get industry_id\n",
    "result_df = pd.merge(companies_industry_df, industries_df, left_on='industry', right_on='industry_name', how='left')\n",
    "\n",
    "# Drop redundant columns and rename 'industry_id' to 'industry_id_new'\n",
    "result_df = result_df.drop(columns=['industry', 'industry_name']).rename(columns={'industry_id': 'industry_id'})\n",
    "\n",
    "industries_df.to_csv('industries.csv', index=False)\n",
    "result_df.to_csv('companies_industry.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Performed data cleaning on 'companies.csv', including removal of dots and dashes in the 'state' column, elimination of accent characters, and consolidation of multiple spaces in the 'state' field, updating the original file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from unidecode import unidecode\n",
    "\n",
    "file_path = 'companies.csv'\n",
    "\n",
    "data_types = {'company_size': pd.Int64Dtype()}  # Force 'company_size' to integer type\n",
    "\n",
    "data = pd.read_csv(file_path, dtype=data_types)\n",
    "\n",
    "# Remove dots (.) and dashes (-) from the 'state' column\n",
    "data['state'] = data['state'].str.replace('[.]', '', regex=True)\n",
    "data['state'] = data['state'].str.replace('[-]', '', regex=True)\n",
    "\n",
    "# Remove accent characters from the 'state' column\n",
    "data['state'] = data['state'].apply(lambda x: unidecode(str(x)))\n",
    "\n",
    "# Convert 'state' column to strings and clean the values\n",
    "data['state'] = data['state'].astype(str).str.strip()\n",
    "\n",
    "# Replace multiple spaces with a single space in the 'state' column only\n",
    "data['state'] = data['state'].str.replace(r'\\s+', ' ', regex=True)\n",
    "\n",
    "data.to_csv(file_path, index=False)\n",
    "\n",
    "print(f\"Modifications complete. Updated data written to '{file_path}'.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removed unnecessary columns 'formatted_work_type' and 'scraped' in 'job_posting.csv'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "file_path = 'job_postings.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Drop the columns 'formatted_work_type' and 'scraped'\n",
    "columns_to_drop = ['formatted_work_type', 'scraped']\n",
    "data.drop(columns=columns_to_drop, inplace=True)\n",
    "\n",
    "data.to_csv(file_path, index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The code reads a CSV file containing job postings, removes emojis from the 'description' column using regex, and saves the modified data directly back to the original file without creating a new CSV file. This process ensures the original file is updated with the modified 'description' column without duplication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import regex as re\n",
    "\n",
    "file_path = 'job_postings.csv'  \n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Function to remove emojis from text\n",
    "def remove_emojis(text):\n",
    "    if isinstance(text, str):\n",
    "        emoji_pattern = re.compile(\"[\"\n",
    "                               u\"\\U0001F600-\\U0001F64F\"  # Emoticons\n",
    "                               u\"\\U0001F300-\\U0001F5FF\"  # Symbols & Pictographs\n",
    "                               u\"\\U0001F680-\\U0001F6FF\"  # Transport & Map Symbols\n",
    "                               u\"\\U0001F700-\\U0001F77F\"  # Alchemical Symbols\n",
    "                               u\"\\U0001F780-\\U0001F7FF\"  # Geometric Shapes Extended\n",
    "                               u\"\\U0001F800-\\U0001F8FF\"  # Supplemental Arrows-C\n",
    "                               u\"\\U0001F900-\\U0001F9FF\"  # Supplemental Symbols and Pictographs\n",
    "                               u\"\\U0001FA00-\\U0001FA6F\"  # Extended People\n",
    "                               u\"\\U0001FA70-\\U0001FAFF\"  # Various Symbols\n",
    "                               u\"\\U00002702-\\U000027B0\"  # Dingbats\n",
    "                               u\"\\U000024C2-\\U0001F251\" \n",
    "                               \"]+\", flags=re.UNICODE)\n",
    "        return emoji_pattern.sub(r'', text)\n",
    "    else:\n",
    "        return str(text)\n",
    "\n",
    "# Apply the function to remove emojis from the 'description' column\n",
    "data['description'] = data['description'].apply(remove_emojis)\n",
    "\n",
    "data.to_csv(file_path, index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Modified the 'company_specialities.csv' file by replacing the 'speciality' column with 'speciality_id'.Mapped company_id to speciality_id based on extracted unique specialties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "companies_df = pd.read_csv('companies.csv')  \n",
    "\n",
    "\n",
    "company_specialities_df = pd.read_csv('company_specialities.csv')\n",
    "\n",
    "# Extract unique specialties and assign IDs\n",
    "unique_specialties = company_specialities_df['speciality'].unique()\n",
    "specialities_mapping = {specialty: idx + 1 for idx, specialty in enumerate(unique_specialties)}\n",
    "\n",
    "specialities_df = pd.DataFrame({'speciality': list(specialities_mapping.keys()),\n",
    "                                'speciality_id': list(specialities_mapping.values())})\n",
    "\n",
    "# Update company_specialities_df by mapping speciality to speciality_id\n",
    "company_specialities_df['speciality_id'] = company_specialities_df['speciality'].map(specialities_mapping)\n",
    "\n",
    "company_specialities_df.drop('speciality', axis=1, inplace=True)\n",
    "\n",
    "# Merge with companies_df to map company_id to speciality_id\n",
    "merged_df = pd.merge(company_specialities_df, companies_df, on='company_id')\n",
    "\n",
    "# Rearrange columns if needed\n",
    "merged_df = merged_df[['company_id', 'speciality_id']]\n",
    "\n",
    "merged_df.to_csv('company_specialities.csv', index=False)\n",
    "\n",
    "specialities_df.to_csv('specialities.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add benefit_id to the benefits.csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "file_path = 'benefits.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Create a new column 'benefit_id' as an incremental ID starting from 1\n",
    "data['benefit_id'] = range(1, len(data) + 1)\n",
    "\n",
    "# Reorder columns to have the new 'benefit_id' as the first column (if desired)\n",
    "cols = data.columns.tolist()\n",
    "cols = ['benefit_id'] + [col for col in cols if col != 'benefit_id']\n",
    "data = data[cols]\n",
    "\n",
    "data.to_csv('benefit_new.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert the date type of timestamp to date for better queries in Job_posting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "df = pd.read_csv(\"job_posting.csv\")\n",
    "\n",
    "# Convert the timestamp to datetime format and replace the existing column\n",
    "df['listed_time'] = pd.to_datetime(df['listed_time'], unit='ms')  \n",
    "df['expiry'] = pd.to_datetime(df['expiry'], unit='ms')  \n",
    "df['original_listed_time'] = pd.to_datetime(df['original_listed_time'], unit='ms')  \n",
    "df['closed_time'] = pd.to_datetime(df['closed_time'], unit='ms')  \n",
    "\n",
    "# Format the 'time_recorded' column to remove the time part\n",
    "df['listed_time'] = df['listed_time'].dt.strftime('%Y-%m-%d')\n",
    "df['expiry'] = df['expiry'].dt.strftime('%Y-%m-%d')\n",
    "df['original_listed_time'] = df['original_listed_time'].dt.strftime('%Y-%m-%d')\n",
    "df['closed_time'] = df['closed_time'].dt.strftime('%Y-%m-%d')\n",
    "\n",
    "df.to_csv(\"job_posting_conv_new.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert the date type of timestamp to date for better queries in employee_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "df = pd.read_csv(\"employee_counts.csv\")\n",
    "\n",
    "# Convert the timestamp to datetime format and replace the existing column\n",
    "df['time_recorded'] = pd.to_datetime(df['time_recorded'], unit='s')\n",
    "\n",
    "# Format the 'time_recorded' column to remove the time part\n",
    "df['time_recorded'] = df['time_recorded'].dt.strftime('%Y-%m-%d')\n",
    "\n",
    "df.to_csv(\"employee_counts_conv.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Populate the Job Posting Ontology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# required libraries\n",
    "import pandas as pd\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "from rdflib import Graph, Literal, RDF, URIRef, Namespace, BNode, OWL, RDFS\n",
    "\n",
    "from rdflib.namespace import FOAF, XSD\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters and URLs\n",
    "path = str(Path(os.path.abspath(os.getcwd())).parent.absolute())\n",
    "jobPostingUrl = 'data/job_posting.csv'\n",
    "\n",
    "companiesUrl = path + '/GraphDB/data/company_details/companies.csv'\n",
    "company_industriesUrl = path + '/GraphDB/data/company_details/company_industries.csv'\n",
    "company_specialitiesUrl = path + '/GraphDB/data/company_details/company_specialities.csv'\n",
    "specialitiesUrl = path + '/GraphDB/data/company_details/specialities.csv'\n",
    "employee_countsUrl = path + '/GraphDB/data/company_details/employee_counts.csv'\n",
    "c = path + '/GraphDB/data/company_details/specialities.csv'\n",
    "\n",
    "benefitsUrl = path + '/GraphDB/data/job_details/benefits.csv'\n",
    "job_industriesUrl = path + '/GraphDB/data/job_details/job_industries.csv'\n",
    "job_skillsUrl = path + '/GraphDB/data/job_details/job_skills.csv'\n",
    "salariesUrl = path + '/GraphDB/data/job_details/salaries.csv'\n",
    "\n",
    "industriesUrl = path + '/GraphDB/data/maps/industries.csv'\n",
    "skillsUrl = path + '/GraphDB/data/maps/skills.csv'\n",
    "\n",
    "\n",
    "# country codes\n",
    "countriesURL = path + '/GraphDB/data/countries/all.csv'\n",
    "\n",
    "# saving folder\n",
    "savePath =  path + '/data/rdf/linkedinDB/' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct the country, Linkedin Job Posting and SKOS ontology namespaces not known by RDFlib\n",
    "CNS = Namespace(\"http://eulersharp.sourceforge.net/2003/03swap/countries#\")\n",
    "LNJP = Namespace(\"http://www.dei.unipd.it/database2/LinkedinJobPosting#\")\n",
    "SKOS = Namespace(\"https://www.w3.org/2009/08/skos-reference/skos-owl1-dl.rdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Job Posting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CSV files in memory\n",
    "jobPosting = pd.read_csv(jobPostingUrl, sep=',', index_col='job_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create the graph\n",
    "g = Graph()\n",
    "\n",
    "# Bind the namespaces to a prefix for more readable output\n",
    "g.bind(\"foaf\", FOAF)\n",
    "g.bind(\"xsd\", XSD)\n",
    "g.bind(\"countries\", CNS)\n",
    "g.bind(\"lnjp\", LNJP)\n",
    "g.bind(\"skos\", SKOS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- saving serialization ---\n",
      "RDF data exported to job_postings.ttl\n",
      "Serialization saved successfully.\n"
     ]
    }
   ],
   "source": [
    "for index, row in jobPosting.iterrows():\n",
    "    # Create the JobPosting node with a URI\n",
    "    jobPostingId = \"job_\" + str(index)\n",
    "    JobPosting = URIRef(LNJP[jobPostingId])\n",
    "    g.add((JobPosting, RDF.type, LNJP.JobPosting))\n",
    "\n",
    "    # Add data properties\n",
    "    g.add((JobPosting, LNJP.job_id, Literal(index, datatype=XSD.integer)))\n",
    "    g.add((JobPosting, LNJP.title, Literal(row['title'], datatype=XSD.string)))\n",
    "    g.add((JobPosting, LNJP.description, Literal(row['description'], datatype=XSD.string)))\n",
    "    if not pd.isnull(row['max_salary']) and row['max_salary'] != \"\":\n",
    "        g.add((JobPosting, LNJP.max_salary, Literal(row['max_salary'], datatype=XSD.decimal)))\n",
    "    if not pd.isnull(row['med_salary']) and row['med_salary'] != \"\":\n",
    "        g.add((JobPosting, LNJP.med_salary, Literal(row['med_salary'], datatype=XSD.decimal)))\n",
    "    if not pd.isnull(row['min_salary']) and row['min_salary'] != \"\":\n",
    "        g.add((JobPosting, LNJP.min_salary, Literal(row['min_salary'], datatype=XSD.decimal)))\n",
    "    if not pd.isnull(row['pay_period']) and row['pay_period'] != \"\":    \n",
    "        g.add((JobPosting, LNJP.pay_period, Literal(row['pay_period'], datatype=XSD.string)))\n",
    "    g.add((JobPosting, LNJP.location, Literal(row['location'], datatype=XSD.string)))\n",
    "    if pd.notnull(row['applies']) and str(row['applies']).strip():\n",
    "        g.add((JobPosting, LNJP.applies, Literal(int(row['applies']), datatype=XSD.integer)))\n",
    "    g.add((JobPosting, LNJP.original_listed_time, Literal(row['original_listed_time'], datatype=XSD.date)))\n",
    "    if not pd.isnull(row['remote_allowed']) and row['remote_allowed'] != \"\":\n",
    "        g.add((JobPosting, LNJP.remote_allowed, Literal(row['remote_allowed'], datatype=XSD.integer)))\n",
    "    if not pd.isnull(row['views']) and row['views'] != \"\":    \n",
    "        g.add((JobPosting, LNJP.views, Literal(int(row['views']), datatype=XSD.integer)))\n",
    "    g.add((JobPosting, LNJP.job_posting_url, Literal(row['job_posting_url'], datatype=XSD.string)))\n",
    "    if not pd.isnull(row['application_url']) and row['application_url'] != \"\":  \n",
    "        g.add((JobPosting, LNJP.application_url, Literal(row['application_url'], datatype=XSD.string)))\n",
    "    g.add((JobPosting, LNJP.application_type, Literal(row['application_type'], datatype=XSD.string)))\n",
    "    g.add((JobPosting, LNJP.expiry, Literal(row['expiry'], datatype=XSD.date)))\n",
    "    if not pd.isnull(row['closed_time']) and row['closed_time'] != \"\":\n",
    "        g.add((JobPosting, LNJP.closed_time, Literal(row['closed_time'], datatype=XSD.date)))\n",
    "    if not pd.isnull(row['formatted_experience_level']) and row['formatted_experience_level'] != \"\":    \n",
    "        g.add((JobPosting, LNJP.formatted_experience_level, Literal(row['formatted_experience_level'], datatype=XSD.string)))\n",
    "    if not pd.isnull(row['skills_desc']) and row['skills_desc'] != \"\":\n",
    "        g.add((JobPosting, LNJP.skills_desc, Literal(row['skills_desc'], datatype=XSD.string)))\n",
    "    g.add((JobPosting, LNJP.listed_time, Literal(row['listed_time'], datatype=XSD.date)))\n",
    "    if not pd.isnull(row['posting_domain']) and row['posting_domain'] != \"\":\n",
    "        g.add((JobPosting, LNJP.posting_domain, Literal(row['posting_domain'], datatype=XSD.string)))\n",
    "    g.add((JobPosting, LNJP.sponsored, Literal(row['sponsored'], datatype=XSD.integer)))\n",
    "    g.add((JobPosting, LNJP.work_type, Literal(row['work_type'], datatype=XSD.string)))\n",
    "    if not pd.isnull(row['currency']) and row['currency'] != \"\":\n",
    "        g.add((JobPosting, LNJP.currency, Literal(row['currency'], datatype=XSD.string)))\n",
    "    if not pd.isnull(row['compensation_type']) and row['compensation_type'] != \"\":\n",
    "        g.add((JobPosting, LNJP.compensation_type, Literal(row['compensation_type'], datatype=XSD.string)))\n",
    "\n",
    "    # Add object properties\n",
    "    if pd.notnull(row['company_id']) and str(row['company_id']).strip():  # Check for NaN and empty string\n",
    "        Company = URIRef(LNJP[\"company_\" + str(int(row['company_id']))])\n",
    "        if Company:  # Checking if Company is not empty\n",
    "            g.add((JobPosting, LNJP['hasCompany'], Company))\n",
    "\n",
    "# Serialize the RDF graph to Turtle format\n",
    "turtle_file_path = 'job_postings.ttl'\n",
    "\n",
    "# Check if the file already exists\n",
    "if os.path.exists(turtle_file_path):\n",
    "    user_input = input(f\"The file '{turtle_file_path}' already exists. Do you want to rewrite it? (y/n): \").lower()\n",
    "    \n",
    "    if user_input != 'y':\n",
    "        print(\"Serialization not saved. Exiting.\")\n",
    "        # You might want to add additional logic or exit the script if the user chooses not to overwrite the file.\n",
    "        exit()\n",
    "\n",
    "# If the file doesn't exist or the user chose to overwrite, proceed with saving the serialization\n",
    "print(\"--- saving serialization ---\")\n",
    "g.serialize(destination=turtle_file_path, format='turtle')\n",
    "\n",
    "print(f\"RDF data exported to {turtle_file_path}\")\n",
    "print(\"Serialization saved successfully.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Skills"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skills DataFrame read successfully.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "skills = pd.read_csv(skillsUrl, sep=',', index_col='skill_abr', keep_default_na=False, na_values=['_'])\n",
    "assert not skills.empty, \"Skills DataFrame is empty. Check the CSV file or URL.\"\n",
    "\n",
    "print(\"Skills DataFrame read successfully.\")\n",
    "\n",
    "g = Graph()\n",
    "\n",
    "\n",
    "g.bind(\"foaf\", FOAF)\n",
    "g.bind(\"xsd\", XSD)\n",
    "g.bind(\"countries\", CNS)\n",
    "g.bind(\"lnjp\", LNJP)\n",
    "g.bind(\"skos\", SKOS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- saving serialization ---\n",
      "RDF data exported to skills.ttl\n",
      "Serialization saved successfully.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for skill_abr, row in skills.iterrows():\n",
    "    \n",
    "    skillId = \"skill_\" + str(skill_abr)\n",
    "    Skill = URIRef(LNJP[skillId])\n",
    "    g.add((Skill, RDF.type, SKOS.Concept))\n",
    "    g.add((Skill, RDF.type, LNJP.Skill))\n",
    "\n",
    "    # Add data properties\n",
    "    g.add((Skill, LNJP.skill_abr, Literal(skill_abr, datatype=XSD.string)))\n",
    "    g.add((Skill, LNJP.skill_name, Literal(row['skill_name'], datatype=XSD.string)))\n",
    "    # Add object properties\n",
    "    if(row['broader_concept']):\n",
    "        broaderSkillId = \"skill_\" + str(row['broader_concept'])\n",
    "        broaderSkill = URIRef(LNJP[broaderSkillId])\n",
    "        g.add((Skill, SKOS.broader, broaderSkill))\n",
    "    \n",
    "    if(row['narrower_concept']):\n",
    "        narrowerSkillId = \"skill_\" + str(row['narrower_concept'])\n",
    "        narrowerSkill = URIRef(LNJP[narrowerSkillId])\n",
    "        g.add((Skill, SKOS.narrower, narrowerSkill))\n",
    "        \n",
    "    if(row['related_concept']):\n",
    "        relatedSkillId = \"skill_\" + str(row['related_concept'])\n",
    "        relatedSkill = URIRef(LNJP[relatedSkillId])\n",
    "        g.add((Skill, SKOS.related, relatedSkill))\n",
    "    \n",
    "\n",
    "\n",
    "turtle_file_path = 'skills.ttl'\n",
    "\n",
    "\n",
    "if os.path.exists(turtle_file_path):\n",
    "    user_input = input(f\"The file '{turtle_file_path}' already exists. Do you want to rewrite it? (y/n): \").lower()\n",
    "\n",
    "    if user_input != 'y':\n",
    "        print(\"Serialization not saved. Exiting.\")\n",
    "        exit()\n",
    "\n",
    "print(\"--- saving serialization ---\")\n",
    "g.serialize(destination=turtle_file_path, format='turtle')\n",
    "\n",
    "print(f\"RDF data exported to {turtle_file_path}\")\n",
    "print(\"Serialization saved successfully.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## JOB_SKILLS JOIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skills DataFrame read successfully.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "job_skills = pd.read_csv(job_skillsUrl, sep=',', index_col='job_id')\n",
    "assert not job_skills.empty, \"Skills DataFrame is empty. Check the CSV file or URL.\"\n",
    "\n",
    "\n",
    "print(\"Skills DataFrame read successfully.\")\n",
    "\n",
    "g = Graph()\n",
    "\n",
    "\n",
    "g.bind(\"foaf\", FOAF)\n",
    "g.bind(\"xsd\", XSD)\n",
    "g.bind(\"countries\", CNS)\n",
    "g.bind(\"lnjp\", LNJP)\n",
    "g.bind(\"skos\", SKOS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- saving serialization ---\n",
      "RDF data exported to job_skills_join.ttl\n",
      "Serialization saved successfully.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for index, row in job_skills.iterrows():\n",
    "    \n",
    "    jobPostingId = \"job_\" + str(index)\n",
    "    JobPosting = URIRef(LNJP[jobPostingId])\n",
    "    skillId = \"skill_\" + str(row['skill_abr'])\n",
    "    Skill = URIRef(LNJP[skillId])\n",
    "    g.add((JobPosting, LNJP['hasSkill'], Skill))\n",
    "\n",
    "turtle_file_path = 'job_skills_join.ttl'\n",
    "\n",
    "\n",
    "if os.path.exists(turtle_file_path):\n",
    "    user_input = input(f\"The file '{turtle_file_path}' already exists. Do you want to rewrite it? (y/n): \").lower()\n",
    "\n",
    "    if user_input != 'y':\n",
    "        print(\"Serialization not saved. Exiting.\")\n",
    "        \n",
    "        exit()\n",
    "\n",
    "\n",
    "print(\"--- saving serialization ---\")\n",
    "g.serialize(destination=turtle_file_path, format='turtle')\n",
    "\n",
    "print(f\"RDF data exported to {turtle_file_path}\")\n",
    "print(\"Serialization saved successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Industry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Industries DataFrame read successfully.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "industries = pd.read_csv(industriesUrl, sep=',', index_col='industry_id', keep_default_na=False, na_values=['_'])\n",
    "assert not industries.empty, \"Industries DataFrame is empty. Check the CSV file or URL.\"\n",
    "\n",
    "print(\"Industries DataFrame read successfully.\")\n",
    "\n",
    "g = Graph()\n",
    "\n",
    "\n",
    "g.bind(\"foaf\", FOAF)\n",
    "g.bind(\"xsd\", XSD)\n",
    "g.bind(\"countries\", CNS)\n",
    "g.bind(\"lnjp\", LNJP)\n",
    "g.bind(\"skos\", SKOS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- saving serialization ---\n",
      "RDF data exported to industries.ttl\n",
      "Serialization saved successfully.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for index, row in industries.iterrows():\n",
    "   \n",
    "    industryId = \"industry_\" + str(index)\n",
    "    Industry = URIRef(LNJP[industryId])\n",
    "    g.add((Industry, RDF.type, LNJP.Industry))\n",
    "\n",
    "    # Add data properties\n",
    "    g.add((Industry, LNJP['industry_id'], Literal(index, datatype=XSD.integer)))\n",
    "    if not pd.isnull(row['industry_name']) and row['industry_name'] != \"\":\n",
    "        g.add((Industry, LNJP['industry_name'], Literal(row['industry_name'], datatype=XSD.string)))\n",
    "    \n",
    "\n",
    "turtle_file_path = 'industries.ttl'\n",
    "\n",
    "\n",
    "if os.path.exists(turtle_file_path):\n",
    "    user_input = input(f\"The file '{turtle_file_path}' already exists. Do you want to rewrite it? (y/n): \").lower()\n",
    "\n",
    "    if user_input != 'y':\n",
    "        print(\"Serialization not saved. Exiting.\")\n",
    "        exit()\n",
    "\n",
    "print(\"--- saving serialization ---\")\n",
    "g.serialize(destination=turtle_file_path, format='turtle')\n",
    "\n",
    "print(f\"RDF data exported to {turtle_file_path}\")\n",
    "print(\"Serialization saved successfully.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Job_Industries join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skills DataFrame read successfully.\n"
     ]
    }
   ],
   "source": [
    "job_industries = pd.read_csv(job_industriesUrl, sep=',', index_col='job_id')\n",
    "assert not job_industries.empty, \"Skills DataFrame is empty. Check the CSV file or URL.\"\n",
    "\n",
    "print(\"Skills DataFrame read successfully.\")\n",
    "g = Graph()\n",
    "\n",
    "g.bind(\"foaf\", FOAF)\n",
    "g.bind(\"xsd\", XSD)\n",
    "g.bind(\"countries\", CNS)\n",
    "g.bind(\"lnjp\", LNJP)\n",
    "g.bind(\"skos\", SKOS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- saving serialization ---\n",
      "RDF data exported to job_industries_join.ttl\n",
      "Serialization saved successfully.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for index, row in job_industries.iterrows():\n",
    "\n",
    "    \n",
    "    jobPosting_id = \"job_\" + str(index)\n",
    "    JobPosting = URIRef(LNJP[jobPosting_id])\n",
    "    Industry = URIRef(LNJP[\"industry_\" + str(row['industry_id'])])\n",
    "    g.add((JobPosting, LNJP['hasIndustryType'], Industry))\n",
    "\n",
    "turtle_file_path = 'job_industries_join.ttl'\n",
    "\n",
    "if os.path.exists(turtle_file_path):\n",
    "    user_input = input(f\"The file '{turtle_file_path}' already exists. Do you want to rewrite it? (y/n): \").lower()\n",
    "\n",
    "    if user_input != 'y':\n",
    "        print(\"Serialization not saved. Exiting.\")\n",
    "        exit()\n",
    "\n",
    "\n",
    "print(\"--- saving serialization ---\")\n",
    "g.serialize(destination=turtle_file_path, format='turtle')\n",
    "\n",
    "print(f\"RDF data exported to {turtle_file_path}\")\n",
    "print(\"Serialization saved successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Company"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Companies DataFrame read successfully.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "companies = pd.read_csv(companiesUrl, sep=',', index_col='company_id', keep_default_na=False, na_values=['_'])\n",
    "assert not companies.empty, \"Companies DataFrame is empty. Check the CSV file or URL.\"\n",
    "\n",
    "print(\"Companies DataFrame read successfully.\")\n",
    "\n",
    "g = Graph()\n",
    "\n",
    "\n",
    "g.bind(\"foaf\", FOAF)\n",
    "g.bind(\"xsd\", XSD)\n",
    "g.bind(\"countries\", CNS)\n",
    "g.bind(\"lnjp\", LNJP)\n",
    "g.bind(\"skos\", SKOS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- saving serialization ---\n",
      "RDF data exported to companies.ttl\n",
      "Serialization saved successfully.\n"
     ]
    }
   ],
   "source": [
    "largeCompany = LNJP.largeCompany\n",
    "company_size_property = LNJP['company_size']\n",
    "\n",
    "BNL0 = BNode()\n",
    "BNL1 = BNode()\n",
    "BNL2 = BNode()\n",
    "BNL3 = BNode()\n",
    "BNL4 = BNode()\n",
    "BNL5 = BNode()\n",
    "BNL6 = BNode()\n",
    "BNL7 = BNode()\n",
    "BNL8 = BNode()\n",
    "\n",
    "g.add((largeCompany, RDF.type, OWL.Class))\n",
    "g.add((largeCompany, RDFS.subClassOf, LNJP.Company))\n",
    "\n",
    "g.add((largeCompany, OWL.equivalentClass, BNL0))\n",
    "\n",
    "g.add((BNL0, OWL.intersectionOf, BNL1))\n",
    "\n",
    "g.add((BNL1, RDF.type, RDF.List))\n",
    "g.add((BNL1, RDF.first, LNJP.Company))\n",
    "g.add((BNL1, RDF.rest, BNL2))\n",
    "\n",
    "g.add((BNL2, RDF.type, RDF.List))\n",
    "g.add((BNL2, RDF.first, BNL3))\n",
    "g.add((BNL2, RDF.rest, RDF.nil))\n",
    "\n",
    "g.add((BNL3, RDF.type, OWL.Restriction))\n",
    "g.add((BNL3, OWL.onProperty, company_size_property))\n",
    "g.add((BNL3, OWL.someValuesFrom, BNL4))\n",
    "\n",
    "g.add((BNL4, RDF.type, RDFS.Datatype))\n",
    "g.add((BNL4, OWL.onDatatype, XSD.integer))\n",
    "g.add((BNL4, OWL.withRestrictions, BNL5))\n",
    "\n",
    "g.add((BNL5, RDF.type, RDF.List))\n",
    "g.add((BNL6, RDF.type, RDF.List))\n",
    "g.add((BNL7, RDF.type, RDF.List))\n",
    "g.add((BNL8, RDF.type, RDF.List))\n",
    "\n",
    "g.add((BNL5, RDF.first, BNL6))\n",
    "g.add((BNL5, RDF.rest, BNL7))\n",
    "\n",
    "g.add((BNL7, RDF.first, BNL8))\n",
    "g.add((BNL7, RDF.rest, RDF.nil))\n",
    "\n",
    "g.add((BNL6, XSD.minInclusive, Literal(\"6\", datatype=XSD.integer)))\n",
    "g.add((BNL8, XSD.maxInclusive, Literal(\"7\", datatype=XSD.integer)))\n",
    "\n",
    "mediumCompany = LNJP.mediumCompany\n",
    "company_size_property = LNJP['company_size']\n",
    "\n",
    "BNM0 = BNode()\n",
    "BNM1 = BNode()\n",
    "BNM2 = BNode()\n",
    "BNM3 = BNode()\n",
    "BNM4 = BNode()\n",
    "BNM5 = BNode()\n",
    "BNM6 = BNode()\n",
    "BNM7 = BNode()\n",
    "BNM8 = BNode()\n",
    "\n",
    "g.add((mediumCompany, RDF.type, OWL.Class))\n",
    "g.add((mediumCompany, RDFS.subClassOf, LNJP.Company))\n",
    "\n",
    "g.add((mediumCompany, OWL.equivalentClass, BNM0))\n",
    "\n",
    "g.add((BNM0, OWL.intersectionOf, BNM1))\n",
    "\n",
    "g.add((BNM1, RDF.type, RDF.List))\n",
    "g.add((BNM1, RDF.first, LNJP.Company))\n",
    "g.add((BNM1, RDF.rest, BNM2))\n",
    "\n",
    "g.add((BNM2, RDF.type, RDF.List))\n",
    "g.add((BNM2, RDF.first, BNM3))\n",
    "g.add((BNM2, RDF.rest, RDF.nil))\n",
    "\n",
    "g.add((BNM3, RDF.type, OWL.Restriction))\n",
    "g.add((BNM3, OWL.onProperty, company_size_property))\n",
    "g.add((BNM3, OWL.someValuesFrom, BNM4))\n",
    "\n",
    "g.add((BNM4, RDF.type, RDFS.Datatype))\n",
    "g.add((BNM4, OWL.onDatatype, XSD.integer))\n",
    "g.add((BNM4, OWL.withRestrictions, BNM5))\n",
    "\n",
    "g.add((BNM5, RDF.type, RDF.List))\n",
    "g.add((BNM6, RDF.type, RDF.List))\n",
    "g.add((BNM7, RDF.type, RDF.List))\n",
    "g.add((BNM8, RDF.type, RDF.List))\n",
    "\n",
    "g.add((BNM5, RDF.first, BNM6))\n",
    "g.add((BNM5, RDF.rest, BNM7))\n",
    "\n",
    "g.add((BNM7, RDF.first, BNM8))\n",
    "g.add((BNM7, RDF.rest, RDF.nil))\n",
    "\n",
    "g.add((BNM6, XSD.minInclusive, Literal(\"3\", datatype=XSD.integer)))\n",
    "g.add((BNM8, XSD.maxInclusive, Literal(\"5\", datatype=XSD.integer)))\n",
    "\n",
    "smallCompany = LNJP.smallCompany\n",
    "company_size_property = LNJP['company_size']\n",
    "\n",
    "BNS0 = BNode()\n",
    "BNS1 = BNode()\n",
    "BNS2 = BNode()\n",
    "BNS3 = BNode()\n",
    "BNS4 = BNode()\n",
    "BNS5 = BNode()\n",
    "BNS6 = BNode()\n",
    "BNS7 = BNode()\n",
    "BNS8 = BNode()\n",
    "\n",
    "g.add((smallCompany, RDF.type, OWL.Class))\n",
    "g.add((smallCompany, RDFS.subClassOf, LNJP.Company))\n",
    "\n",
    "g.add((smallCompany, OWL.equivalentClass, BNS0))\n",
    "\n",
    "g.add((BNS0, OWL.intersectionOf, BNS1))\n",
    "\n",
    "g.add((BNS1, RDF.type, RDF.List))\n",
    "g.add((BNS1, RDF.first, LNJP.Company))\n",
    "g.add((BNS1, RDF.rest, BNS2))\n",
    "\n",
    "g.add((BNS2, RDF.type, RDF.List))\n",
    "g.add((BNS2, RDF.first, BNS3))\n",
    "g.add((BNS2, RDF.rest, RDF.nil))\n",
    "\n",
    "g.add((BNS3, RDF.type, OWL.Restriction))\n",
    "g.add((BNS3, OWL.onProperty, company_size_property))\n",
    "g.add((BNS3, OWL.someValuesFrom, BNS4))\n",
    "\n",
    "g.add((BNS4, RDF.type, RDFS.Datatype))\n",
    "g.add((BNS4, OWL.onDatatype, XSD.integer))\n",
    "g.add((BNS4, OWL.withRestrictions, BNS5))\n",
    "\n",
    "g.add((BNS5, RDF.type, RDF.List))\n",
    "g.add((BNS6, RDF.type, RDF.List))\n",
    "g.add((BNS7, RDF.type, RDF.List))\n",
    "g.add((BNS8, RDF.type, RDF.List))\n",
    "\n",
    "g.add((BNS5, RDF.first, BNS6))\n",
    "g.add((BNS5, RDF.rest, BNS7))\n",
    "\n",
    "g.add((BNS7, RDF.first, BNS8))\n",
    "g.add((BNS7, RDF.rest, RDF.nil))\n",
    "\n",
    "g.add((BNS6, XSD.minInclusive, Literal(\"1\", datatype=XSD.integer)))\n",
    "g.add((BNS8, XSD.maxInclusive, Literal(\"2\", datatype=XSD.integer)))\n",
    "\n",
    "for index, row in companies.iterrows():\n",
    "    \n",
    "    companyId = \"company_\" + str(index)\n",
    "    Company = URIRef(LNJP[companyId])\n",
    "    g.add((Company, RDF.type, LNJP.Company))\n",
    "\n",
    "    # Add data properties\n",
    "    g.add((Company, LNJP['company_id'], Literal(int(index), datatype=XSD.integer)))\n",
    "    g.add((Company, LNJP['name'], Literal(row['name'], datatype=XSD.string)))\n",
    "    if not pd.isnull(row['description']) and row['description'] != \"\":\n",
    "        g.add((Company, LNJP['description'], Literal(row['description'], datatype=XSD.string)))\n",
    "    if not pd.isnull(row['company_size']) and row['company_size'] != \"\":\n",
    "        g.add((Company, LNJP['company_size'], Literal(row['company_size'], datatype=XSD.integer)))\n",
    "        if (Literal(row['company_size'], datatype=XSD.integer) >= Literal(\"6\", datatype=XSD.integer)) and (Literal(row['company_size'], datatype=XSD.integer) <= Literal(\"7\", datatype=XSD.integer)):\n",
    "            g.add((Company,RDF.type,LNJP.largeCompany))\n",
    "        if (Literal(row['company_size'], datatype=XSD.integer) >= Literal(\"3\", datatype=XSD.integer)) and (Literal(row['company_size'], datatype=XSD.integer) <= Literal(\"5\", datatype=XSD.integer)):\n",
    "            g.add((Company,RDF.type,LNJP.mediumCompany))\n",
    "        if (Literal(row['company_size'], datatype=XSD.integer) >= Literal(\"1\", datatype=XSD.integer)) and (Literal(row['company_size'], datatype=XSD.integer) <= Literal(\"2\", datatype=XSD.integer)):\n",
    "            g.add((Company,RDF.type,LNJP.smallCompany))\n",
    "    if not pd.isnull(row['state']) and row['state'] != \"\":    \n",
    "        g.add((Company, LNJP['state'], Literal(row['state'], datatype=XSD.string)))\n",
    "    g.add((Company, LNJP['city'], Literal(row['city'], datatype=XSD.string)))\n",
    "    if not pd.isnull(row['zip_code']) and row['zip_code'] != \"\":   \n",
    "        g.add((Company, LNJP['zip_code'], Literal(row['zip_code'], datatype=XSD.string)))\n",
    "    if not pd.isnull(row['address']) and row['address'] != \"\":    \n",
    "        g.add((Company, LNJP['address'], Literal(row['address'], datatype=XSD.string)))\n",
    "    g.add((Company, LNJP['url'], Literal(row['url'], datatype=XSD.string)))\n",
    "\n",
    "    # Add object properties\n",
    "    Country = URIRef(CNS[row['country']])\n",
    "    g.add((Company, LNJP['hasCountry'], Country))\n",
    "\n",
    "turtle_file_path = 'companies.ttl'\n",
    "\n",
    "\n",
    "if os.path.exists(turtle_file_path):\n",
    "    user_input = input(f\"The file '{turtle_file_path}' already exists. Do you want to rewrite it? (y/n): \").lower()\n",
    "\n",
    "    if user_input != 'y':\n",
    "        print(\"Serialization not saved. Exiting.\")\n",
    "        exit()\n",
    "\n",
    "print(\"--- saving serialization ---\")\n",
    "g.serialize(destination=turtle_file_path, format='turtle')\n",
    "\n",
    "print(f\"RDF data exported to {turtle_file_path}\")\n",
    "print(\"Serialization saved successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Company_Industries join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skills DataFrame read successfully.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "company_industries = pd.read_csv(company_industriesUrl, sep=',', index_col='company_id', keep_default_na=False, na_values=['_'])\n",
    "assert not company_industries.empty, \"Skills DataFrame is empty. Check the CSV file or URL.\"\n",
    "\n",
    "print(\"Skills DataFrame read successfully.\")\n",
    "g = Graph()\n",
    "\n",
    "g.bind(\"foaf\", FOAF)\n",
    "g.bind(\"xsd\", XSD)\n",
    "g.bind(\"countries\", CNS)\n",
    "g.bind(\"lnjp\", LNJP)\n",
    "g.bind(\"skos\", SKOS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- saving serialization ---\n",
      "RDF data exported to company_industries_join.ttl\n",
      "Serialization saved successfully.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for index, row in company_industries.iterrows():\n",
    "    company_id = \"company_\" + str(index)\n",
    "    Company = URIRef(LNJP[company_id])\n",
    "    Industry = URIRef(LNJP[\"industry_\" + str(row['industry_id'])])\n",
    "    g.add((Company, LNJP['hasIndustry'], Industry))\n",
    "    \n",
    "\n",
    "turtle_file_path = 'company_industries_join.ttl'\n",
    "\n",
    "if os.path.exists(turtle_file_path):\n",
    "    user_input = input(f\"The file '{turtle_file_path}' already exists. Do you want to rewrite it? (y/n): \").lower()\n",
    "\n",
    "    if user_input != 'y':\n",
    "        print(\"Serialization not saved. Exiting.\")\n",
    "        exit()\n",
    "\n",
    "print(\"--- saving serialization ---\")\n",
    "g.serialize(destination=turtle_file_path, format='turtle')\n",
    "\n",
    "print(f\"RDF data exported to {turtle_file_path}\")\n",
    "print(\"Serialization saved successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Person Counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "employee_counts = pd.read_csv(employee_countsUrl, sep=',', index_col='company_record_id')\n",
    "\n",
    "g = Graph()\n",
    "\n",
    "g.bind(\"foaf\", FOAF)\n",
    "g.bind(\"xsd\", XSD)\n",
    "g.bind(\"countries\", CNS)\n",
    "g.bind(\"lnjp\", LNJP)\n",
    "g.bind(\"skos\", SKOS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- saving serialization ---\n",
      "RDF data exported to employee_counts.ttl\n",
      "Serialization saved successfully.\n"
     ]
    }
   ],
   "source": [
    "highFollowers = LNJP.highFollowers\n",
    "follower_count_property = LNJP['follower_count']\n",
    "\n",
    "BNH0 = BNode()\n",
    "BNH1 = BNode()\n",
    "BNH2 = BNode()\n",
    "BNH3 = BNode()\n",
    "BNH4 = BNode()\n",
    "BNH5 = BNode()\n",
    "BNH6 = BNode()\n",
    "BNH7 = BNode()\n",
    "BNH8 = BNode()\n",
    "\n",
    "g.add((highFollowers, RDF.type, OWL.Class))\n",
    "g.add((highFollowers, RDFS.subClassOf, LNJP.Record))\n",
    "\n",
    "g.add((highFollowers, OWL.equivalentClass, BNH0))\n",
    "\n",
    "g.add((BNH0, OWL.intersectionOf, BNH1))\n",
    "\n",
    "g.add((BNH1, RDF.type, RDF.List))\n",
    "g.add((BNH1, RDF.first, LNJP.Record))\n",
    "g.add((BNH1, RDF.rest, BNH2))\n",
    "\n",
    "g.add((BNH2, RDF.type, RDF.List))\n",
    "g.add((BNH2, RDF.first, BNH3))\n",
    "g.add((BNH2, RDF.rest, RDF.nil))\n",
    "\n",
    "g.add((BNH3, RDF.type, OWL.Restriction))\n",
    "g.add((BNH3, OWL.onProperty, follower_count_property))\n",
    "g.add((BNH3, OWL.someValuesFrom, BNH4))\n",
    "\n",
    "g.add((BNH4, RDF.type, RDFS.Datatype))\n",
    "g.add((BNH4, OWL.onDatatype, XSD.integer))\n",
    "g.add((BNH4, OWL.withRestrictions, BNH5))\n",
    "\n",
    "g.add((BNH5, RDF.type, RDF.List))\n",
    "g.add((BNH6, RDF.type, RDF.List))\n",
    "g.add((BNH7, RDF.type, RDF.List))\n",
    "g.add((BNH8, RDF.type, RDF.List))\n",
    "\n",
    "g.add((BNH5, RDF.first, BNH6))\n",
    "g.add((BNH5, RDF.rest, BNH7))\n",
    "\n",
    "g.add((BNH7, RDF.first, BNH8))\n",
    "g.add((BNH7, RDF.rest, RDF.nil))\n",
    "\n",
    "g.add((BNH6, XSD.minInclusive, Literal(\"201083\", datatype=XSD.integer)))\n",
    "\n",
    "lowFollowers = LNJP.lowFollowers\n",
    "follower_count_property = LNJP['follower_count']\n",
    "\n",
    "BNL0 = BNode()\n",
    "BNL1 = BNode()\n",
    "BNL2 = BNode()\n",
    "BNL3 = BNode()\n",
    "BNL4 = BNode()\n",
    "BNL5 = BNode()\n",
    "BNL6 = BNode()\n",
    "BNL7 = BNode()\n",
    "BNL8 = BNode()\n",
    "\n",
    "g.add((lowFollowers, RDF.type, OWL.Class))\n",
    "g.add((lowFollowers, RDFS.subClassOf, LNJP.Record))\n",
    "\n",
    "g.add((lowFollowers, OWL.equivalentClass, BNL0))\n",
    "\n",
    "g.add((BNL0, OWL.intersectionOf, BNL1))\n",
    "\n",
    "g.add((BNL1, RDF.type, RDF.List))\n",
    "g.add((BNL1, RDF.first, LNJP.Record))\n",
    "g.add((BNL1, RDF.rest, BNL2))\n",
    "\n",
    "g.add((BNL2, RDF.type, RDF.List))\n",
    "g.add((BNL2, RDF.first, BNL3))\n",
    "g.add((BNL2, RDF.rest, RDF.nil))\n",
    "\n",
    "g.add((BNL3, RDF.type, OWL.Restriction))\n",
    "g.add((BNL3, OWL.onProperty, follower_count_property))\n",
    "g.add((BNL3, OWL.someValuesFrom, BNL4))\n",
    "\n",
    "g.add((BNL4, RDF.type, RDFS.Datatype))\n",
    "g.add((BNL4, OWL.onDatatype, XSD.integer))\n",
    "g.add((BNL4, OWL.withRestrictions, BNL5))\n",
    "\n",
    "g.add((BNL5, RDF.type, RDF.List))\n",
    "g.add((BNL6, RDF.type, RDF.List))\n",
    "g.add((BNL7, RDF.type, RDF.List))\n",
    "g.add((BNL8, RDF.type, RDF.List))\n",
    "\n",
    "g.add((BNL5, RDF.first, BNL6))\n",
    "g.add((BNL5, RDF.rest, BNL7))\n",
    "\n",
    "g.add((BNL7, RDF.first, BNL8))\n",
    "g.add((BNL7, RDF.rest, RDF.nil))\n",
    "\n",
    "g.add((BNL6, XSD.maxExclusive, Literal(\"201083\", datatype=XSD.integer)))\n",
    "\n",
    "for index, row in employee_counts.iterrows():\n",
    "    company_record_id = \"company_record_\" + str(index)\n",
    "    Record = URIRef(LNJP[company_record_id])\n",
    "    g.add((Record, RDF.type, LNJP.Record))\n",
    "\n",
    "    # Add data properties\n",
    "    g.add((Record, LNJP['company_record_id'], Literal(index, datatype=XSD.integer)))\n",
    "    g.add((Record, LNJP['employee_count'], Literal(int(row['employee_count']), datatype=XSD.integer)))\n",
    "    g.add((Record, LNJP['follower_count'], Literal(int(row['follower_count']), datatype=XSD.integer)))\n",
    "    g.add((Record, LNJP['time_recorded'], Literal(row['time_recorded'], datatype=XSD.date)))\n",
    "\n",
    "    if (Literal(row['follower_count'], datatype=XSD.integer) >= Literal(\"201083\", datatype=XSD.integer)):\n",
    "        g.add((Record,RDF.type,LNJP.highFollowers))\n",
    "    if (Literal(row['follower_count'], datatype=XSD.integer) < Literal(\"201083\", datatype=XSD.integer)):\n",
    "        g.add((Record,RDF.type,LNJP.lowFollowers))\n",
    "\n",
    "    # Add object properties\n",
    "    Company = URIRef(LNJP[\"company_\" + str(int(row['company_id']))])\n",
    "    g.add((Record, LNJP['IsForCompany'], Company))\n",
    "\n",
    "turtle_file_path = 'employee_counts.ttl'\n",
    "\n",
    "\n",
    "if os.path.exists(turtle_file_path):\n",
    "    user_input = input(f\"The file '{turtle_file_path}' already exists. Do you want to rewrite it? (y/n): \").lower()\n",
    "\n",
    "    if user_input != 'y':\n",
    "        print(\"Serialization not saved. Exiting.\")\n",
    "        exit()\n",
    "\n",
    "print(\"--- saving serialization ---\")\n",
    "g.serialize(destination=turtle_file_path, format='turtle')\n",
    "\n",
    "print(f\"RDF data exported to {turtle_file_path}\")\n",
    "print(\"Serialization saved successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benefits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "benefits = pd.read_csv(benefitsUrl, sep=',', index_col='benefit_id')\n",
    "\n",
    "g = Graph()\n",
    "\n",
    "g.bind(\"foaf\", FOAF)\n",
    "g.bind(\"xsd\", XSD)\n",
    "g.bind(\"countries\", CNS)\n",
    "g.bind(\"lnjp\", LNJP)\n",
    "g.bind(\"skos\", SKOS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- saving serialization ---\n",
      "RDF data exported to benefits.ttl\n",
      "Serialization saved successfully.\n"
     ]
    }
   ],
   "source": [
    "for index, row in benefits.iterrows():\n",
    "    benefit_id = \"benefit_\" + str(index)\n",
    "    Benefit = URIRef(LNJP[benefit_id])\n",
    "    g.add((Benefit, RDF.type, LNJP.Benefit))\n",
    "\n",
    "    # Add data properties\n",
    "    g.add((Benefit, LNJP['benefit_id'], Literal(index, datatype=XSD.integer)))\n",
    "    g.add((Benefit, LNJP['inferred'], Literal(row['inferred'], datatype=XSD.boolean)))\n",
    "    g.add((Benefit, LNJP['type'], Literal(row['type'], datatype=XSD.string)))\n",
    "\n",
    "    # Add object properties\n",
    "    JobPosting = URIRef(LNJP[\"job_\" + str(row['job_id'])])\n",
    "    g.add((Benefit, LNJP['isForJobPosting'], JobPosting))\n",
    "    \n",
    "turtle_file_path = 'benefits.ttl'\n",
    "if os.path.exists(turtle_file_path):\n",
    "    user_input = input(f\"The file '{turtle_file_path}' already exists. Do you want to rewrite it? (y/n): \").lower()\n",
    "\n",
    "    if user_input != 'y':\n",
    "        print(\"Serialization not saved. Exiting.\")\n",
    "        exit()\n",
    "print(\"--- saving serialization ---\")\n",
    "g.serialize(destination=turtle_file_path, format='turtle')\n",
    "\n",
    "print(f\"RDF data exported to {turtle_file_path}\")\n",
    "print(\"Serialization saved successfully.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specialities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "specialities = pd.read_csv(specialitiesUrl, sep=',', index_col='speciality_id', keep_default_na=False, na_values=['_'])\n",
    "\n",
    "g = Graph()\n",
    "\n",
    "g.bind(\"foaf\", FOAF)\n",
    "g.bind(\"xsd\", XSD)\n",
    "g.bind(\"countries\", CNS)\n",
    "g.bind(\"lnjp\", LNJP)\n",
    "g.bind(\"skos\", SKOS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- saving serialization ---\n",
      "RDF data exported to speciality.ttl\n",
      "Serialization saved successfully.\n"
     ]
    }
   ],
   "source": [
    "for index, row in specialities.iterrows():\n",
    "    speciality_id = \"speciality_\" + str(index)\n",
    "    Speciality = URIRef(LNJP[speciality_id])\n",
    "    g.add((Speciality, RDF.type, LNJP.Speciality))\n",
    "\n",
    "    g.add((Speciality, LNJP['speciality_id'], Literal(index, datatype=XSD.integer)))\n",
    "    g.add((Speciality, LNJP['speciality'], Literal(row['speciality'], datatype=XSD.string)))\n",
    "turtle_file_path = 'speciality.ttl'\n",
    "\n",
    "if os.path.exists(turtle_file_path):\n",
    "    user_input = input(f\"The file '{turtle_file_path}' already exists. Do you want to rewrite it? (y/n): \").lower()\n",
    "\n",
    "    if user_input != 'y':\n",
    "        print(\"Serialization not saved. Exiting.\")\n",
    "        exit()\n",
    "\n",
    "print(\"--- saving serialization ---\")\n",
    "g.serialize(destination=turtle_file_path, format='turtle')\n",
    "\n",
    "print(f\"RDF data exported to {turtle_file_path}\")\n",
    "print(\"Serialization saved successfully.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Company_Specialities Join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "company_specialities = pd.read_csv(company_specialitiesUrl, sep=',', index_col='company_id', keep_default_na=False, na_values=['_'])\n",
    "\n",
    "g = Graph()\n",
    "\n",
    "g.bind(\"foaf\", FOAF)\n",
    "g.bind(\"xsd\", XSD)\n",
    "g.bind(\"countries\", CNS)\n",
    "g.bind(\"lnjp\", LNJP)\n",
    "g.bind(\"skos\", SKOS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- saving serialization ---\n",
      "RDF data exported to company_specialities_join.ttl\n",
      "Serialization saved successfully.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for index, row in company_specialities.iterrows():\n",
    "    \n",
    "    company_id = \"company_\" + str(index)\n",
    "    Company = URIRef(LNJP[company_id])\n",
    "    Speciality = URIRef(LNJP[\"speciality_\" + str(row['speciality_id'])])\n",
    "    g.add((Company, LNJP['hasSpeciality'], Speciality))\n",
    "    \n",
    "turtle_file_path = 'company_specialities_join.ttl'\n",
    "\n",
    "if os.path.exists(turtle_file_path):\n",
    "    user_input = input(f\"The file '{turtle_file_path}' already exists. Do you want to rewrite it? (y/n): \").lower()\n",
    "\n",
    "    if user_input != 'y':\n",
    "        print(\"Serialization not saved. Exiting.\")\n",
    "        exit()\n",
    "print(\"--- saving serialization ---\")\n",
    "g.serialize(destination=turtle_file_path, format='turtle')\n",
    "\n",
    "print(f\"RDF data exported to {turtle_file_path}\")\n",
    "print(\"Serialization saved successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Salaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "salaries = pd.read_csv(salariesUrl, sep=',', index_col='salary_id')\n",
    "\n",
    "g = Graph()\n",
    "\n",
    "g.bind(\"foaf\", FOAF)\n",
    "g.bind(\"xsd\", XSD)\n",
    "g.bind(\"countries\", CNS)\n",
    "g.bind(\"lnjp\", LNJP)\n",
    "g.bind(\"skos\", SKOS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- saving serialization ---\n",
      "RDF data exported to salaries.ttl\n",
      "Serialization saved successfully.\n"
     ]
    }
   ],
   "source": [
    "for index, row in salaries.iterrows():\n",
    "    salary_id = \"salary_\" + str(index)\n",
    "    Salary = URIRef(LNJP[salary_id])\n",
    "    g.add((Salary, RDF.type, LNJP.Salary))\n",
    "\n",
    "    # Add data properties\n",
    "    g.add((Salary, LNJP['salary_id'], Literal(index, datatype=XSD.integer)))\n",
    "    if not pd.isnull(row['max_salary']) and row['max_salary'] != \"\":\n",
    "        g.add((Salary, LNJP['max_salary'], Literal(row['max_salary'], datatype=XSD.decimal)))\n",
    "    if not pd.isnull(row['med_salary']) and row['med_salary'] != \"\":        \n",
    "        g.add((Salary, LNJP['med_salary'], Literal(row['med_salary'], datatype=XSD.decimal)))\n",
    "    if not pd.isnull(row['min_salary']) and row['min_salary'] != \"\":    \n",
    "        g.add((Salary, LNJP['min_salary'], Literal(row['min_salary'], datatype=XSD.decimal)))\n",
    "    g.add((Salary, LNJP['pay_period'], Literal(row['pay_period'], datatype=XSD.string)))\n",
    "    g.add((Salary, LNJP['currency'], Literal(row['currency'], datatype=XSD.string)))\n",
    "    g.add((Salary, LNJP['compensation_type'], Literal(row['compensation_type'], datatype=XSD.string)))\n",
    "\n",
    "    # Add object properties\n",
    "    JobPosting = URIRef(LNJP[\"job_\" + str(row['job_id'])])\n",
    "    g.add((Salary, LNJP['isAllocatedFor'], JobPosting))\n",
    "turtle_file_path = 'salaries.ttl'\n",
    "\n",
    "if os.path.exists(turtle_file_path):\n",
    "    user_input = input(f\"The file '{turtle_file_path}' already exists. Do you want to rewrite it? (y/n): \").lower()\n",
    "\n",
    "    if user_input != 'y':\n",
    "        print(\"Serialization not saved. Exiting.\")\n",
    "        exit()\n",
    "\n",
    "print(\"--- saving serialization ---\")\n",
    "g.serialize(destination=turtle_file_path, format='turtle')\n",
    "\n",
    "print(f\"RDF data exported to {turtle_file_path}\")\n",
    "print(\"Serialization saved successfully.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
